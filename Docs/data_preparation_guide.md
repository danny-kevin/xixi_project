# æ•°æ®å‡†å¤‡æ¨¡å—ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æ•°æ®å‡†å¤‡æ¨¡å—æä¾›äº†å®Œæ•´çš„ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- å¤šæºå¼‚æ„æ•°æ®åŠ è½½
- æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
- æ—¶é—´åºåˆ—çª—å£æ„å»º
- PyTorch Dataset å’Œ DataLoader åˆ›å»º

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install pandas numpy torch scikit-learn openpyxl
```

### 2. å‡†å¤‡æ•°æ®

#### æ–¹å¼1: ä½¿ç”¨ç¤ºä¾‹æ•°æ®

```bash
# ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•
python scripts/generate_sample_data.py
```

è¿™å°†åœ¨ `data/raw/` ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
- `epidemic.csv` - ç–«æƒ…æ•°æ®
- `mobility.csv` - äººå£æµåŠ¨æ•°æ®
- `environmental.csv` - ç¯å¢ƒæ•°æ®
- `intervention.csv` - å¹²é¢„æ”¿ç­–æ•°æ®

#### æ–¹å¼2: ä½¿ç”¨çœŸå®æ•°æ®

å°†æ‚¨çš„æ•°æ®æ–‡ä»¶æ”¾åœ¨ `data/raw/` ç›®å½•ä¸‹ï¼Œç¡®ä¿ï¼š
- æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å« `date` åˆ—ï¼ˆæ—¥æœŸæ ¼å¼ï¼‰
- ç–«æƒ…æ•°æ®è‡³å°‘åŒ…å« `new_cases` åˆ—
- æ–‡ä»¶æ ¼å¼ä¸º CSV æˆ– Excel

### 3. ä½¿ç”¨æ•°æ®ç®¡é“

#### æœ€ç®€å•çš„æ–¹å¼ - ä¸€é”®å‡†å¤‡æ•°æ®

```python
from src.data import prepare_data

# ä¸€é”®å®Œæˆæ‰€æœ‰æ•°æ®å‡†å¤‡å·¥ä½œ
train_loader, val_loader, test_loader, preprocessor = prepare_data(
    data_dir='data',
    window_size=21,      # è¾“å…¥çª—å£ï¼š21å¤©
    horizon=7,           # é¢„æµ‹èŒƒå›´ï¼š7å¤©
    batch_size=32,
    num_workers=0        # Windowsä¸Šè®¾ç½®ä¸º0
)

# ç°åœ¨å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒ
for batch_x, batch_y in train_loader:
    # batch_x: (batch_size, 21, num_features)
    # batch_y: (batch_size, 7)
    print(f"è¾“å…¥å½¢çŠ¶: {batch_x.shape}, ç›®æ ‡å½¢çŠ¶: {batch_y.shape}")
    break
```

#### æ›´çµæ´»çš„æ–¹å¼ - ä½¿ç”¨ DataPipeline

```python
from src.data import DataPipeline

# åˆ›å»ºæ•°æ®ç®¡é“
pipeline = DataPipeline(
    data_dir='data',
    window_size=21,
    horizon=7,
    train_ratio=0.7,
    val_ratio=0.15,
    batch_size=32,
    num_workers=0
)

# è¿è¡Œå®Œæ•´æµç¨‹
train_loader, val_loader, test_loader = pipeline.run()

# è·å–é¢„å¤„ç†å™¨ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰
preprocessor = pipeline.get_preprocessor()

# è·å–ç‰¹å¾åç§°
feature_names = pipeline.get_feature_names()
print(f"ç‰¹å¾åˆ—è¡¨: {feature_names}")
```

#### åˆ†æ­¥éª¤ä½¿ç”¨ - å®Œå…¨è‡ªå®šä¹‰

```python
from src.data import DataLoader, DataPreprocessor, EpidemicDataset, create_data_loaders

# 1. åŠ è½½æ•°æ®
loader = DataLoader('data')
data_dict = loader.load_all_data()
merged_data = loader.merge_data_sources(data_dict)

# 2. é¢„å¤„ç†
preprocessor = DataPreprocessor()

# å¤„ç†ç¼ºå¤±å€¼
cleaned_data = preprocessor.handle_missing_values(merged_data, method='interpolate')

# æ£€æµ‹å¼‚å¸¸å€¼
outliers = preprocessor.detect_outliers(cleaned_data, method='iqr', threshold=1.5)

# å½’ä¸€åŒ–
normalized_data = preprocessor.normalize(cleaned_data, method='minmax')

# 3. åˆ›å»ºæ—¶é—´çª—å£
data_array = normalized_data.values
X, y = preprocessor.create_time_windows(
    data_array,
    window_size=21,
    horizon=7,
    stride=1
)

# 4. åˆ’åˆ†æ•°æ®é›†
n_samples = len(X)
train_end = int(n_samples * 0.7)
val_end = int(n_samples * 0.85)

X_train, y_train = X[:train_end], y[:train_end]
X_val, y_val = X[train_end:val_end], y[train_end:val_end]
X_test, y_test = X[val_end:], y[val_end:]

# 5. åˆ›å»º Dataset
train_dataset = EpidemicDataset(X_train, y_train)
val_dataset = EpidemicDataset(X_val, y_val)
test_dataset = EpidemicDataset(X_test, y_test)

# 6. åˆ›å»º DataLoader
train_loader, val_loader, test_loader = create_data_loaders(
    train_dataset, val_dataset, test_dataset,
    batch_size=32,
    num_workers=0
)
```

## ğŸ“Š æ•°æ®æ ¼å¼è¯´æ˜

### è¾“å…¥æ•°æ®æ ¼å¼

æ¯ä¸ªæ•°æ®æ–‡ä»¶åº”åŒ…å«ä»¥ä¸‹åˆ—ï¼š

#### ç–«æƒ…æ•°æ® (epidemic.csv)
```
date,new_cases,new_deaths,new_recovered,cumulative_cases,cumulative_deaths,active_cases
2020-01-01,100,2,80,100,2,18
2020-01-02,120,3,90,220,5,45
...
```

#### äººå£æµåŠ¨æ•°æ® (mobility.csv)
```
date,intra_city_flow,inter_city_flow,public_transport,retail_mobility,workplace_mobility
2020-01-01,100,80,90,70,85
2020-01-02,95,75,85,65,80
...
```

#### ç¯å¢ƒæ•°æ® (environmental.csv)
```
date,temperature,humidity,uv_index,precipitation,wind_speed
2020-01-01,15.5,60,5,0,3.2
2020-01-02,16.2,58,6,0,2.8
...
```

#### å¹²é¢„æ”¿ç­–æ•°æ® (intervention.csv)
```
date,lockdown_level,social_distance,mask_mandate,vaccination_rate,testing_rate
2020-01-01,0,0,0,0,5.2
2020-01-02,0,0,0,0,5.5
...
```

### è¾“å‡ºæ•°æ®æ ¼å¼

```python
# DataLoader è¾“å‡º
for batch_x, batch_y in train_loader:
    # batch_x: torch.Tensor, shape=(batch_size, window_size, num_features)
    #   - batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚32ï¼‰
    #   - window_size: è¾“å…¥çª—å£å¤§å°ï¼ˆå¦‚21å¤©ï¼‰
    #   - num_features: ç‰¹å¾æ•°é‡ï¼ˆæ‰€æœ‰æ•°æ®æºçš„ç‰¹å¾æ€»å’Œï¼‰
    
    # batch_y: torch.Tensor, shape=(batch_size, horizon) æˆ– (batch_size,)
    #   - horizon: é¢„æµ‹èŒƒå›´ï¼ˆå¦‚7å¤©ï¼‰
    #   - ç›®æ ‡å˜é‡é€šå¸¸æ˜¯ new_cases
    pass
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰ç¼ºå¤±å€¼å¤„ç†

```python
preprocessor = DataPreprocessor()

# æ–¹æ³•1: æ—¶é—´åºåˆ—æ’å€¼ï¼ˆæ¨èï¼‰
df = preprocessor.handle_missing_values(df, method='interpolate')

# æ–¹æ³•2: å‰å‘å¡«å……
df = preprocessor.handle_missing_values(df, method='ffill')

# æ–¹æ³•3: åå‘å¡«å……
df = preprocessor.handle_missing_values(df, method='bfill')

# æ–¹æ³•4: å‡å€¼å¡«å……
df = preprocessor.handle_missing_values(df, method='mean')
```

### 2. å¼‚å¸¸å€¼æ£€æµ‹

```python
# IQR æ–¹æ³•ï¼ˆå››åˆ†ä½è·ï¼‰
outliers = preprocessor.detect_outliers(df, method='iqr', threshold=1.5)

# Z-score æ–¹æ³•
outliers = preprocessor.detect_outliers(df, method='zscore', threshold=3.0)

# æŸ¥çœ‹å¼‚å¸¸å€¼
print(f"å¼‚å¸¸å€¼æ•°é‡: {outliers.sum().sum()}")
```

### 3. æ•°æ®å½’ä¸€åŒ–

```python
# MinMax å½’ä¸€åŒ– [0, 1]
df_normalized = preprocessor.normalize(df, method='minmax')

# æ ‡å‡†åŒ– (å‡å€¼=0, æ ‡å‡†å·®=1)
df_normalized = preprocessor.normalize(df, method='standard')

# æŒ‡å®šç‰¹å®šåˆ—è¿›è¡Œå½’ä¸€åŒ–
df_normalized = preprocessor.normalize(
    df, 
    method='minmax',
    columns=['new_cases', 'temperature']
)
```

### 4. åå½’ä¸€åŒ–

```python
# è®­ç»ƒåï¼Œå°†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦
predictions_normalized = model(input_data)  # å½’ä¸€åŒ–çš„é¢„æµ‹ç»“æœ

# åå½’ä¸€åŒ–
predictions_original = preprocessor.inverse_transform(
    predictions_normalized.cpu().numpy(),
    column='epidemic_new_cases'  # ä½¿ç”¨åˆå¹¶åçš„åˆ—å
)
```

### 5. è‡ªå®šä¹‰æ—¶é—´çª—å£

```python
# åˆ›å»ºä¸åŒå¤§å°çš„çª—å£
X_14, y_14 = preprocessor.create_time_windows(
    data, 
    window_size=14,   # 14å¤©è¾“å…¥
    horizon=3,        # 3å¤©é¢„æµ‹
    stride=1          # æ¯æ¬¡æ»‘åŠ¨1å¤©
)

# ä½¿ç”¨æ›´å¤§çš„æ­¥é•¿ï¼ˆå‡å°‘æ ·æœ¬æ•°é‡ï¼‰
X_sparse, y_sparse = preprocessor.create_time_windows(
    data,
    window_size=21,
    horizon=7,
    stride=7          # æ¯æ¬¡æ»‘åŠ¨7å¤©
)
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### è¿è¡Œå®Œæ•´æµ‹è¯•

```bash
# æµ‹è¯•æ‰€æœ‰ç»„ä»¶
python scripts/test_data_pipeline.py
```

### éªŒè¯æ•°æ®å½¢çŠ¶

```python
from src.data import prepare_data

train_loader, val_loader, test_loader, preprocessor = prepare_data(
    data_dir='data',
    window_size=21,
    horizon=7,
    batch_size=32
)

# æ£€æŸ¥æ•°æ®å½¢çŠ¶
for batch_x, batch_y in train_loader:
    print(f"âœ“ è¾“å…¥å½¢çŠ¶: {batch_x.shape}")  # åº”è¯¥æ˜¯ (32, 21, num_features)
    print(f"âœ“ ç›®æ ‡å½¢çŠ¶: {batch_y.shape}")  # åº”è¯¥æ˜¯ (32, 7) æˆ– (32,)
    print(f"âœ“ æ•°æ®ç±»å‹: {batch_x.dtype}")  # åº”è¯¥æ˜¯ torch.float32
    break
```

## âš ï¸ å¸¸è§é—®é¢˜

### 1. ModuleNotFoundError: No module named 'torch'

**è§£å†³æ–¹æ¡ˆ**: å®‰è£… PyTorch
```bash
pip install torch
```

### 2. æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨æ­£ç¡®çš„ä½ç½®
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls data/raw/

# å¦‚æœæ²¡æœ‰ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®
python scripts/generate_sample_data.py
```

### 3. æ—¥æœŸè§£æé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿æ—¥æœŸåˆ—åä¸º `date`ï¼Œæ ¼å¼ä¸ºæ ‡å‡†æ—¥æœŸæ ¼å¼
```python
# å¦‚æœæ—¥æœŸæ ¼å¼ç‰¹æ®Šï¼Œå¯ä»¥æ‰‹åŠ¨æŒ‡å®š
df = pd.read_csv('data.csv', parse_dates=['date'], date_parser=lambda x: pd.to_datetime(x, format='%Y/%m/%d'))
```

### 4. å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**: å‡å°æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ›´å°‘çš„ç‰¹å¾
```python
train_loader, val_loader, test_loader, preprocessor = prepare_data(
    data_dir='data',
    batch_size=16,    # å‡å°æ‰¹æ¬¡å¤§å°
    num_workers=0     # å‡å°‘å·¥ä½œè¿›ç¨‹
)
```

### 5. Windows ä¸Š DataLoader æŠ¥é”™

**è§£å†³æ–¹æ¡ˆ**: è®¾ç½® `num_workers=0`
```python
train_loader, val_loader, test_loader, preprocessor = prepare_data(
    data_dir='data',
    num_workers=0  # Windows ä¸Šå¿…é¡»è®¾ç½®ä¸º 0
)
```

## ğŸ“š API å‚è€ƒ

### DataLoader

```python
DataLoader(data_dir: str)
```
- `load_epidemic_data(filename)` - åŠ è½½ç–«æƒ…æ•°æ®
- `load_mobility_data(filename)` - åŠ è½½äººå£æµåŠ¨æ•°æ®
- `load_environmental_data(filename)` - åŠ è½½ç¯å¢ƒæ•°æ®
- `load_intervention_data(filename)` - åŠ è½½å¹²é¢„æ”¿ç­–æ•°æ®
- `load_all_data()` - åŠ è½½æ‰€æœ‰æ•°æ®
- `merge_data_sources(data_dict)` - åˆå¹¶å¤šæºæ•°æ®

### DataPreprocessor

```python
DataPreprocessor(config: Optional[Dict] = None)
```
- `handle_missing_values(df, method)` - å¤„ç†ç¼ºå¤±å€¼
- `detect_outliers(df, method, threshold)` - æ£€æµ‹å¼‚å¸¸å€¼
- `normalize(df, method, columns)` - æ•°æ®å½’ä¸€åŒ–
- `create_time_windows(data, window_size, horizon, stride)` - åˆ›å»ºæ—¶é—´çª—å£
- `temporal_train_test_split(data, train_ratio, val_ratio)` - æ—¶åºæ•°æ®åˆ’åˆ†
- `inverse_transform(data, column)` - åå½’ä¸€åŒ–

### EpidemicDataset

```python
EpidemicDataset(X, y, feature_names=None, transform=None)
```
- `__len__()` - è¿”å›æ•°æ®é›†å¤§å°
- `__getitem__(idx)` - è·å–å•ä¸ªæ ·æœ¬
- `get_feature_dim()` - è¿”å›ç‰¹å¾ç»´åº¦
- `get_window_size()` - è¿”å›æ—¶é—´çª—å£å¤§å°

### DataPipeline

```python
DataPipeline(
    data_dir,
    window_size=21,
    horizon=7,
    train_ratio=0.7,
    val_ratio=0.15,
    batch_size=32,
    num_workers=4
)
```
- `load_data()` - åŠ è½½æ•°æ®
- `preprocess_data(df, handle_missing, detect_outliers, normalize)` - é¢„å¤„ç†æ•°æ®
- `create_datasets(df)` - åˆ›å»ºæ•°æ®é›†
- `create_dataloaders(train_dataset, val_dataset, test_dataset)` - åˆ›å»ºDataLoader
- `run()` - è¿è¡Œå®Œæ•´ç®¡é“
- `get_preprocessor()` - è·å–é¢„å¤„ç†å™¨
- `get_feature_names()` - è·å–ç‰¹å¾åç§°

## ğŸ¯ æœ€ä½³å®è·µ

1. **å§‹ç»ˆä½¿ç”¨æ—¶åºåˆ’åˆ†**: é¿å…æ•°æ®æ³„éœ²
2. **ä¿å­˜é¢„å¤„ç†å™¨**: ç”¨äºé¢„æµ‹æ—¶çš„æ•°æ®è½¬æ¢
3. **éªŒè¯æ•°æ®å½¢çŠ¶**: ç¡®ä¿ä¸æ¨¡å‹è¾“å…¥åŒ¹é…
4. **å¤„ç†ç¼ºå¤±å€¼**: åœ¨å½’ä¸€åŒ–ä¹‹å‰
5. **è®°å½•é¢„å¤„ç†æ­¥éª¤**: ä¾¿äºå¤ç°å’Œè°ƒè¯•

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
- `Docs/data_flow.md` - æ•°æ®æµæ–‡æ¡£
- `scripts/test_data_pipeline.py` - æµ‹è¯•ç¤ºä¾‹
- `agents/01_data_preparation_agent.md` - Agent æ–‡æ¡£
