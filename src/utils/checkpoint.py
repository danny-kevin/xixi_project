"""
æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†
Checkpoint Manager

æä¾›æ¨¡å‹ä¿å­˜å’ŒåŠ è½½çš„ç»Ÿä¸€æ¥å£
"""

import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, Optional, Any
import logging

logger = logging.getLogger(__name__)


def save_checkpoint(
    model: nn.Module,
    path: str,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[Any] = None,
    epoch: Optional[int] = None,
    metrics: Optional[Dict] = None,
    **kwargs
) -> None:
    """
    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        path: ä¿å­˜è·¯å¾„
        optimizer: ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        epoch: å½“å‰epochï¼ˆå¯é€‰ï¼‰
        metrics: è¯„ä¼°æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        **kwargs: å…¶ä»–è¦ä¿å­˜çš„å†…å®¹
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_path = Path(path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    # æ„å»ºæ£€æŸ¥ç‚¹å­—å…¸
    checkpoint = {
        'model_state_dict': model.state_dict(),
    }
    
    if optimizer is not None:
        checkpoint['optimizer_state_dict'] = optimizer.state_dict()
    
    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    if epoch is not None:
        checkpoint['epoch'] = epoch
    
    if metrics is not None:
        checkpoint['metrics'] = metrics
    
    # æ·»åŠ é¢å¤–å‚æ•°
    checkpoint.update(kwargs)
    
    # ä¿å­˜
    torch.save(checkpoint, path)
    logger.info(f'âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}')


def load_checkpoint(
    model: nn.Module,
    path: str,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[Any] = None,
    device: Optional[torch.device] = None,
    strict: bool = True
) -> Dict:
    """
    åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        path: æ£€æŸ¥ç‚¹è·¯å¾„
        optimizer: ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        device: ç›®æ ‡è®¾å¤‡ï¼ˆå¯é€‰ï¼‰
        strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…æ¨¡å‹å‚æ•°
        
    Returns:
        æ£€æŸ¥ç‚¹å­—å…¸ï¼ˆåŒ…å«epochã€metricsç­‰ä¿¡æ¯ï¼‰
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(path).exists():
        raise FileNotFoundError(f'æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {path}')
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if device is None:
        checkpoint = torch.load(path)
    else:
        checkpoint = torch.load(path, map_location=device)
    
    # åŠ è½½æ¨¡å‹å‚æ•°
    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)
    logger.info(f'âœ… æ¨¡å‹å‚æ•°å·²åŠ è½½: {path}')
    
    # åŠ è½½ä¼˜åŒ–å™¨å‚æ•°
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        logger.info('âœ… ä¼˜åŒ–å™¨å‚æ•°å·²åŠ è½½')
    
    # åŠ è½½è°ƒåº¦å™¨å‚æ•°
    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.info('âœ… è°ƒåº¦å™¨å‚æ•°å·²åŠ è½½')
    
    # è¿”å›å…¶ä»–ä¿¡æ¯
    info = {}
    if 'epoch' in checkpoint:
        info['epoch'] = checkpoint['epoch']
        logger.info(f'æ£€æŸ¥ç‚¹epoch: {info["epoch"]}')
    
    if 'metrics' in checkpoint:
        info['metrics'] = checkpoint['metrics']
        logger.info(f'æ£€æŸ¥ç‚¹æŒ‡æ ‡: {info["metrics"]}')
    
    return info


def save_model_only(model: nn.Module, path: str) -> None:
    """
    ä»…ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆä¸åŒ…å«ä¼˜åŒ–å™¨ç­‰ï¼‰
    
    Args:
        model: æ¨¡å‹
        path: ä¿å­˜è·¯å¾„
    """
    save_path = Path(path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    torch.save(model.state_dict(), path)
    logger.info(f'âœ… æ¨¡å‹å‚æ•°å·²ä¿å­˜: {path}')


def load_model_only(
    model: nn.Module,
    path: str,
    device: Optional[torch.device] = None,
    strict: bool = True
) -> None:
    """
    ä»…åŠ è½½æ¨¡å‹å‚æ•°
    
    Args:
        model: æ¨¡å‹
        path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: ç›®æ ‡è®¾å¤‡
        strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…
    """
    if not Path(path).exists():
        raise FileNotFoundError(f'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}')
    
    if device is None:
        state_dict = torch.load(path)
    else:
        state_dict = torch.load(path, map_location=device)
    
    model.load_state_dict(state_dict, strict=strict)
    logger.info(f'âœ… æ¨¡å‹å‚æ•°å·²åŠ è½½: {path}')


class CheckpointManager:
    """
    æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    
    è‡ªåŠ¨ç®¡ç†æ¨¡å‹æ£€æŸ¥ç‚¹çš„ä¿å­˜å’Œæ¸…ç†
    """
    
    def __init__(
        self,
        checkpoint_dir: str,
        max_checkpoints: int = 5,
        save_best_only: bool = False
    ):
        """
        åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        
        Args:
            checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
            max_checkpoints: æœ€å¤šä¿ç•™çš„æ£€æŸ¥ç‚¹æ•°é‡
            save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_checkpoints = max_checkpoints
        self.save_best_only = save_best_only
        self.best_metric = float('inf')
        
        logger.info(f'æ£€æŸ¥ç‚¹ç›®å½•: {self.checkpoint_dir}')
    
    def save(
        self,
        model: nn.Module,
        epoch: int,
        metric: float,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        is_best: bool = False
    ) -> None:
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹
            epoch: å½“å‰epoch
            metric: è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚éªŒè¯æŸå¤±ï¼‰
            optimizer: ä¼˜åŒ–å™¨
            scheduler: è°ƒåº¦å™¨
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        # å¦‚æœåªä¿å­˜æœ€ä½³æ¨¡å‹ä¸”å½“å‰ä¸æ˜¯æœ€ä½³ï¼Œåˆ™è·³è¿‡
        if self.save_best_only and not is_best:
            return
        
        # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹
        checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pth'
        save_checkpoint(
            model, str(checkpoint_path),
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=epoch,
            metrics={'metric': metric}
        )
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
        if is_best:
            best_path = self.checkpoint_dir / 'best_model.pth'
            save_checkpoint(
                model, str(best_path),
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                metrics={'metric': metric}
            )
            self.best_metric = metric
            logger.info(f'ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! æŒ‡æ ‡: {metric:.4f}')
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self._cleanup_old_checkpoints()
    
    def _cleanup_old_checkpoints(self) -> None:
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        # è·å–æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoints = sorted(
            self.checkpoint_dir.glob('checkpoint_epoch_*.pth'),
            key=lambda p: p.stat().st_mtime
        )
        
        # ä¿ç•™æœ€æ–°çš„max_checkpointsä¸ª
        if len(checkpoints) > self.max_checkpoints:
            for old_checkpoint in checkpoints[:-self.max_checkpoints]:
                old_checkpoint.unlink()
                logger.debug(f'åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_checkpoint.name}')
    
    def load_best(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        device: Optional[torch.device] = None
    ) -> Dict:
        """
        åŠ è½½æœ€ä½³æ¨¡å‹
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            scheduler: è°ƒåº¦å™¨
            device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            æ£€æŸ¥ç‚¹ä¿¡æ¯
        """
        best_path = self.checkpoint_dir / 'best_model.pth'
        return load_checkpoint(
            model, str(best_path),
            optimizer=optimizer,
            scheduler=scheduler,
            device=device
        )


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
    model = nn.Linear(10, 5)
    optimizer = torch.optim.Adam(model.parameters())
    
    # ç¤ºä¾‹1: åŸºæœ¬ä¿å­˜å’ŒåŠ è½½
    print('ç¤ºä¾‹1: åŸºæœ¬ä¿å­˜å’ŒåŠ è½½')
    save_checkpoint(
        model, 'checkpoints/test.pth',
        optimizer=optimizer,
        epoch=10,
        metrics={'loss': 0.5}
    )
    
    # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½
    new_model = nn.Linear(10, 5)
    new_optimizer = torch.optim.Adam(new_model.parameters())
    info = load_checkpoint(new_model, 'checkpoints/test.pth', optimizer=new_optimizer)
    print(f'åŠ è½½çš„ä¿¡æ¯: {info}')
    
    # ç¤ºä¾‹2: ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    print('\nç¤ºä¾‹2: ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨')
    manager = CheckpointManager('checkpoints/managed', max_checkpoints=3)
    
    for epoch in range(5):
        metric = 1.0 / (epoch + 1)  # æ¨¡æ‹Ÿé€’å‡çš„æŸå¤±
        is_best = metric < manager.best_metric
        manager.save(model, epoch, metric, optimizer, is_best=is_best)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    info = manager.load_best(new_model, new_optimizer)
    print(f'æœ€ä½³æ¨¡å‹ä¿¡æ¯: {info}')
    
    print('\nâœ… æ£€æŸ¥ç‚¹ç®¡ç†æµ‹è¯•å®Œæˆ')
