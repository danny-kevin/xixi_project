"""
æ•°æ®é¢„å¤„ç†ç®¡é“
Data Preprocessing Pipeline

æ•´åˆæ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€æ•°æ®é›†åˆ›å»ºçš„å®Œæ•´æµç¨‹
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import torch
from torch.utils.data import DataLoader as TorchDataLoader

from .data_loader import DataLoader
from .preprocessor import DataPreprocessor
from .dataset import EpidemicDataset, create_data_loaders


class DataPipeline:
    """
    ç«¯åˆ°ç«¯æ•°æ®é¢„å¤„ç†ç®¡é“
    
    æ•´åˆäº†æ•°æ®åŠ è½½ã€æ¸…æ´—ã€é¢„å¤„ç†ã€æ•°æ®é›†æ„å»ºçš„å®Œæ•´æµç¨‹
    """
    
    def __init__(
        self,
        data_dir: str,
        window_size: int = 21,
        horizon: int = 7,
        train_ratio: float = 0.7,
        val_ratio: float = 0.15,
        batch_size: int = 32,
        num_workers: int = 4
    ):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            window_size: è¾“å…¥æ—¶é—´çª—å£å¤§å°ï¼ˆå¤©ï¼‰
            horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
        """
        self.data_dir = Path(data_dir)
        self.window_size = window_size
        self.horizon = horizon
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.batch_size = batch_size
        self.num_workers = num_workers
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.loader = DataLoader(data_dir)
        self.preprocessor = DataPreprocessor()
        
        # å­˜å‚¨å¤„ç†åçš„æ•°æ®
        self.merged_data: Optional[pd.DataFrame] = None
        self.feature_names: List[str] = []
        
    def load_data(self) -> pd.DataFrame:
        """
        åŠ è½½æ‰€æœ‰æ•°æ®æº
        
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        data_dict = self.loader.load_all_data()
        
        if not data_dict:
            raise ValueError("æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®æº")
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data_dict)} ä¸ªæ•°æ®æº")
        
        # åˆå¹¶æ•°æ®
        print("ğŸ”— åˆå¹¶æ•°æ®æº...")
        merged_data = self.loader.merge_data_sources(data_dict)
        print(f"âœ… åˆå¹¶å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {merged_data.shape}")
        
        self.merged_data = merged_data
        return merged_data
    
    def preprocess_data(
        self,
        df: Optional[pd.DataFrame] = None,
        handle_missing: bool = True,
        detect_outliers: bool = True,
        normalize: bool = True
    ) -> pd.DataFrame:
        """
        é¢„å¤„ç†æ•°æ®
        
        Args:
            df: è¾“å…¥DataFrameï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨self.merged_dataï¼‰
            handle_missing: æ˜¯å¦å¤„ç†ç¼ºå¤±å€¼
            detect_outliers: æ˜¯å¦æ£€æµ‹å¼‚å¸¸å€¼
            normalize: æ˜¯å¦å½’ä¸€åŒ–
            
        Returns:
            é¢„å¤„ç†åçš„DataFrame
        """
        if df is None:
            if self.merged_data is None:
                raise ValueError("è¯·å…ˆè°ƒç”¨load_data()åŠ è½½æ•°æ®")
            df = self.merged_data
        
        df_processed = df.copy()
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        if handle_missing:
            print("ğŸ”§ å¤„ç†ç¼ºå¤±å€¼...")
            missing_count = df_processed.isnull().sum().sum()
            if missing_count > 0:
                print(f"   å‘ç° {missing_count} ä¸ªç¼ºå¤±å€¼")
                df_processed = self.preprocessor.handle_missing_values(
                    df_processed, 
                    method='interpolate'
                )
                print(f"   âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
        
        # 2. æ£€æµ‹å¼‚å¸¸å€¼
        if detect_outliers:
            print("ğŸ” æ£€æµ‹å¼‚å¸¸å€¼...")
            outliers = self.preprocessor.detect_outliers(
                df_processed, 
                method='iqr', 
                threshold=1.5
            )
            outlier_count = outliers.sum().sum()
            if outlier_count > 0:
                print(f"   å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼")
                # è¿™é‡Œå¯ä»¥é€‰æ‹©å¤„ç†å¼‚å¸¸å€¼ï¼Œæš‚æ—¶åªè®°å½•
        
        # 3. æ•°æ®å½’ä¸€åŒ–
        if normalize:
            print("ğŸ“ æ•°æ®å½’ä¸€åŒ–...")
            df_processed = self.preprocessor.normalize(
                df_processed,
                method='minmax'
            )
            print(f"   âœ… å½’ä¸€åŒ–å®Œæˆ")
        
        self.feature_names = df_processed.columns.tolist()
        return df_processed
    
    def create_datasets(
        self,
        df: Optional[pd.DataFrame] = None
    ) -> Tuple[EpidemicDataset, EpidemicDataset, EpidemicDataset]:
        """
        åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†
        
        Args:
            df: é¢„å¤„ç†åçš„DataFrame
            
        Returns:
            (train_dataset, val_dataset, test_dataset)
        """
        if df is None:
            raise ValueError("è¯·æä¾›é¢„å¤„ç†åçš„æ•°æ®")
        
        print("ğŸ”¨ åˆ›å»ºæ—¶é—´åºåˆ—çª—å£...")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        data_array = df.values
        
        # åˆ›å»ºæ—¶é—´çª—å£
        X, y = self.preprocessor.create_time_windows(
            data_array,
            window_size=self.window_size,
            horizon=self.horizon,
            stride=1
        )
        
        print(f"   è¾“å…¥å½¢çŠ¶: {X.shape}, ç›®æ ‡å½¢çŠ¶: {y.shape}")
        
        # æ—¶åºåˆ’åˆ†
        print("âœ‚ï¸ åˆ’åˆ†æ•°æ®é›†...")
        n_samples = len(X)
        train_end = int(n_samples * self.train_ratio)
        val_end = int(n_samples * (self.train_ratio + self.val_ratio))
        
        X_train, y_train = X[:train_end], y[:train_end]
        X_val, y_val = X[train_end:val_end], y[train_end:val_end]
        X_test, y_test = X[val_end:], y[val_end:]
        
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # åˆ›å»ºDatasetå¯¹è±¡
        train_dataset = EpidemicDataset(X_train, y_train, self.feature_names)
        val_dataset = EpidemicDataset(X_val, y_val, self.feature_names)
        test_dataset = EpidemicDataset(X_test, y_test, self.feature_names)
        
        return train_dataset, val_dataset, test_dataset
    
    def create_dataloaders(
        self,
        train_dataset: EpidemicDataset,
        val_dataset: EpidemicDataset,
        test_dataset: EpidemicDataset
    ) -> Tuple[TorchDataLoader, TorchDataLoader, TorchDataLoader]:
        """
        åˆ›å»ºDataLoader
        
        Args:
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            test_dataset: æµ‹è¯•æ•°æ®é›†
            
        Returns:
            (train_loader, val_loader, test_loader)
        """
        print("ğŸš€ åˆ›å»ºDataLoader...")
        
        train_loader, val_loader, test_loader = create_data_loaders(
            train_dataset,
            val_dataset,
            test_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers
        )
        
        print(f"   æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        print(f"   æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
        return train_loader, val_loader, test_loader
    
    def run(self) -> Tuple[TorchDataLoader, TorchDataLoader, TorchDataLoader]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å‡†å¤‡ç®¡é“
        
        Returns:
            (train_loader, val_loader, test_loader)
        """
        print("=" * 60)
        print("ğŸ¯ å¼€å§‹æ•°æ®é¢„å¤„ç†ç®¡é“")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        merged_data = self.load_data()
        
        # 2. é¢„å¤„ç†
        processed_data = self.preprocess_data(
            merged_data,
            handle_missing=True,
            detect_outliers=True,
            normalize=True
        )
        
        # 3. åˆ›å»ºæ•°æ®é›†
        train_dataset, val_dataset, test_dataset = self.create_datasets(processed_data)
        
        # 4. åˆ›å»ºDataLoader
        train_loader, val_loader, test_loader = self.create_dataloaders(
            train_dataset,
            val_dataset,
            test_dataset
        )
        
        print("=" * 60)
        print("âœ… æ•°æ®é¢„å¤„ç†ç®¡é“å®Œæˆï¼")
        print("=" * 60)
        
        return train_loader, val_loader, test_loader
    
    def get_preprocessor(self) -> DataPreprocessor:
        """è·å–é¢„å¤„ç†å™¨ï¼ˆç”¨äºåå½’ä¸€åŒ–ç­‰æ“ä½œï¼‰"""
        return self.preprocessor
    
    def get_feature_names(self) -> List[str]:
        """è·å–ç‰¹å¾åç§°åˆ—è¡¨"""
        return self.feature_names


# ä¾¿æ·å‡½æ•°
def prepare_data(
    data_dir: str,
    window_size: int = 21,
    horizon: int = 7,
    batch_size: int = 32,
    **kwargs
) -> Tuple[TorchDataLoader, TorchDataLoader, TorchDataLoader, DataPreprocessor]:
    """
    ä¸€é”®å‡†å¤‡æ•°æ®çš„ä¾¿æ·å‡½æ•°
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        window_size: è¾“å…¥çª—å£å¤§å°
        horizon: é¢„æµ‹èŒƒå›´
        batch_size: æ‰¹æ¬¡å¤§å°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        (train_loader, val_loader, test_loader, preprocessor)
    """
    pipeline = DataPipeline(
        data_dir=data_dir,
        window_size=window_size,
        horizon=horizon,
        batch_size=batch_size,
        **kwargs
    )
    
    train_loader, val_loader, test_loader = pipeline.run()
    preprocessor = pipeline.get_preprocessor()
    
    return train_loader, val_loader, test_loader, preprocessor
