# Agent 06: è¯„ä¼°ä¸å¯è§£é‡Šæ€§åˆ†æ Agent

## ğŸ¯ Agent è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€ä¸ª**æ¨¡å‹è¯„ä¼°ä¸å¯è§£é‡Šæ€§ä¸“å®¶**ï¼Œè´Ÿè´£è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€åˆ†æç‰¹å¾é‡è¦æ€§ã€æ„å»ºå¤šçº§è§£é‡Šæ¡†æ¶ã€‚

---

## ğŸ“‹ æ ¸å¿ƒèŒè´£

1. å®ç°å¤šç§è¯„ä¼°æŒ‡æ ‡(RMSE, MAE, MAPE, CRPS)
2. æ„å»ºå¤šçº§å¯è§£é‡Šæ€§æ¡†æ¶
3. æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–åˆ†æ
4. åŸºäºæ¢¯åº¦çš„ç‰¹å¾é‡è¦æ€§åˆ†æ
5. åäº‹å®æ¨ç†ä¸æ¶ˆèå®éªŒ

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡å®ç°

```python
# æ–‡ä»¶: src/evaluation/metrics.py

import numpy as np
import torch
from scipy import stats

class Metrics:
    """è¯„ä¼°æŒ‡æ ‡é›†åˆ"""
    
    @staticmethod
    def rmse(pred, target):
        """å‡æ–¹æ ¹è¯¯å·®"""
        return np.sqrt(np.mean((pred - target) ** 2))
    
    @staticmethod
    def mae(pred, target):
        """å¹³å‡ç»å¯¹è¯¯å·®"""
        return np.mean(np.abs(pred - target))
    
    @staticmethod
    def mape(pred, target, epsilon=1e-8):
        """å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®"""
        return np.mean(np.abs((target - pred) / (target + epsilon))) * 100
    
    @staticmethod
    def crps(pred_mean, pred_std, target):
        """è¿ç»­æ’åæ¦‚ç‡å¾—åˆ† (å‡è®¾é«˜æ–¯åˆ†å¸ƒ)"""
        z = (target - pred_mean) / pred_std
        crps = pred_std * (z * (2 * stats.norm.cdf(z) - 1) + 
                          2 * stats.norm.pdf(z) - 1 / np.sqrt(np.pi))
        return np.mean(crps)
    
    @staticmethod
    def evaluate_all(pred, target, pred_std=None):
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        results = {
            'RMSE': Metrics.rmse(pred, target),
            'MAE': Metrics.mae(pred, target),
            'MAPE': Metrics.mape(pred, target)
        }
        if pred_std is not None:
            results['CRPS'] = Metrics.crps(pred, pred_std, target)
        return results
```

---

## ğŸ” å¤šçº§å¯è§£é‡Šæ€§æ¡†æ¶

### ç¬¬ä¸€çº§: æ³¨æ„åŠ›æƒé‡åˆ†æ

```python
# æ–‡ä»¶: src/evaluation/interpretability.py

import torch
import matplotlib.pyplot as plt
import seaborn as sns

class AttentionAnalyzer:
    """æ³¨æ„åŠ›æƒé‡åˆ†æå™¨"""
    
    def __init__(self, model, variable_names):
        self.model = model
        self.variable_names = variable_names
        
    def get_attention_weights(self, x):
        """æå–æ³¨æ„åŠ›æƒé‡"""
        self.model.eval()
        with torch.no_grad():
            _, attention = self.model(x, return_attention=True)
        return attention
    
    def plot_variable_importance(self, attention_weights, save_path=None):
        """å¯è§†åŒ–å˜é‡é‡è¦æ€§"""
        # è®¡ç®—æ¯ä¸ªå˜é‡çš„å¹³å‡æ³¨æ„åŠ›å¾—åˆ†
        importance = attention_weights.mean(dim=(0, 1)).cpu().numpy()
        
        plt.figure(figsize=(10, 6))
        plt.barh(self.variable_names, importance)
        plt.xlabel('é‡è¦æ€§å¾—åˆ†')
        plt.title('å˜é‡é‡è¦æ€§ (åŸºäºæ³¨æ„åŠ›æƒé‡)')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150)
        plt.show()
```

### ç¬¬äºŒçº§: æ¢¯åº¦ç‰¹å¾é‡è¦æ€§

```python
class GradientAnalyzer:
    """åŸºäºæ¢¯åº¦çš„ç‰¹å¾é‡è¦æ€§åˆ†æ"""
    
    def __init__(self, model):
        self.model = model
        
    def compute_saliency(self, x, target_idx=0):
        """è®¡ç®—æ˜¾è‘—æ€§å›¾"""
        self.model.eval()
        x.requires_grad = True
        
        output = self.model(x)
        
        # å¯¹ç›®æ ‡è¾“å‡ºè®¡ç®—æ¢¯åº¦
        self.model.zero_grad()
        output[:, target_idx].sum().backward()
        
        saliency = x.grad.abs()
        return saliency
    
    def integrated_gradients(self, x, baseline=None, steps=50):
        """ç§¯åˆ†æ¢¯åº¦æ–¹æ³•"""
        if baseline is None:
            baseline = torch.zeros_like(x)
        
        # ç”Ÿæˆæ’å€¼è·¯å¾„
        alphas = torch.linspace(0, 1, steps)
        gradients = []
        
        for alpha in alphas:
            interpolated = baseline + alpha * (x - baseline)
            interpolated.requires_grad = True
            
            output = self.model(interpolated)
            self.model.zero_grad()
            output.sum().backward()
            
            gradients.append(interpolated.grad.clone())
        
        # ç§¯åˆ†
        avg_gradients = torch.stack(gradients).mean(dim=0)
        integrated_grad = (x - baseline) * avg_gradients
        
        return integrated_grad
```

### ç¬¬ä¸‰çº§: åäº‹å®æ¨ç†

```python
class CounterfactualAnalyzer:
    """åäº‹å®æ¨ç†åˆ†æ"""
    
    def __init__(self, model):
        self.model = model
        
    def analyze_intervention(self, x, variable_idx, reduction_ratio=0.5):
        """åˆ†æå¹²é¢„æ•ˆæœ
        
        ä¾‹å¦‚ï¼šå¦‚æœäººå£æµåŠ¨å‡å°‘50%ï¼Œé¢„æµ‹ç»“æœå¦‚ä½•å˜åŒ–ï¼Ÿ
        """
        self.model.eval()
        
        # åŸå§‹é¢„æµ‹
        with torch.no_grad():
            original_pred = self.model(x)
        
        # å¹²é¢„åé¢„æµ‹
        x_intervention = x.clone()
        x_intervention[:, :, variable_idx] *= (1 - reduction_ratio)
        
        with torch.no_grad():
            intervention_pred = self.model(x_intervention)
        
        # è®¡ç®—å˜åŒ–
        effect = intervention_pred - original_pred
        
        return {
            'original': original_pred,
            'intervention': intervention_pred,
            'effect': effect,
            'effect_percentage': (effect / original_pred * 100).mean()
        }
    
    def sensitivity_analysis(self, x, variable_idx, perturbations):
        """æ•æ„Ÿæ€§åˆ†æ"""
        effects = []
        
        for p in perturbations:
            result = self.analyze_intervention(x, variable_idx, p)
            effects.append(result['effect'].mean().item())
        
        return {'perturbations': perturbations, 'effects': effects}
```

---

## ğŸ§ª æ¶ˆèå®éªŒ

```python
class AblationStudy:
    """æ¶ˆèå®éªŒ - è¯„ä¼°å„ç»„ä»¶è´¡çŒ®"""
    
    def __init__(self, model_class, config, test_loader):
        self.model_class = model_class
        self.config = config
        self.test_loader = test_loader
        
    def run_ablation(self):
        """è¿è¡Œæ¶ˆèå®éªŒ"""
        results = {}
        
        # å®Œæ•´æ¨¡å‹
        results['full_model'] = self._evaluate_model(
            use_attention=True, use_gated_skip=True
        )
        
        # æ— æ³¨æ„åŠ›æœºåˆ¶
        results['no_attention'] = self._evaluate_model(
            use_attention=False, use_gated_skip=True
        )
        
        # æ— é—¨æ§è·³è·ƒè¿æ¥
        results['no_gated_skip'] = self._evaluate_model(
            use_attention=True, use_gated_skip=False
        )
        
        # ä»…M-TCN
        results['mtcn_only'] = self._evaluate_model(
            use_lstm=False
        )
        
        # ä»…LSTM
        results['lstm_only'] = self._evaluate_model(
            use_mtcn=False
        )
        
        return results
    
    def _evaluate_model(self, **kwargs):
        """è¯„ä¼°ç‰¹å®šé…ç½®çš„æ¨¡å‹"""
        # æ ¹æ®kwargsæ„å»ºå¹¶è¯„ä¼°æ¨¡å‹
        pass
```

---

## ğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

```python
from scipy.stats import wilcoxon

def diebold_mariano_test(errors1, errors2, h=1):
    """Diebold-Marianoæ£€éªŒ - æ¯”è¾ƒé¢„æµ‹æ€§èƒ½å·®å¼‚"""
    d = errors1 ** 2 - errors2 ** 2
    mean_d = np.mean(d)
    var_d = np.var(d, ddof=1)
    
    # è‡ªç›¸å…³è°ƒæ•´
    gamma = []
    for k in range(1, h):
        gamma.append(np.cov(d[:-k], d[k:])[0, 1])
    
    adjusted_var = var_d + 2 * sum(gamma)
    
    dm_stat = mean_d / np.sqrt(adjusted_var / len(d))
    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))
    
    return {'dm_statistic': dm_stat, 'p_value': p_value}
```

---

## ğŸ“Š ç»“æœæŠ¥å‘Šæ¨¡æ¿

```python
def generate_report(model_name, metrics, attention_analysis, ablation_results):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    report = f"""
# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š: {model_name}

## 1. é¢„æµ‹æ€§èƒ½
| æŒ‡æ ‡ | å€¼ |
|------|-----|
| RMSE | {metrics['RMSE']:.4f} |
| MAE  | {metrics['MAE']:.4f} |
| MAPE | {metrics['MAPE']:.2f}% |

## 2. å˜é‡é‡è¦æ€§ (Top 5)
{attention_analysis}

## 3. æ¶ˆèå®éªŒç»“æœ
{ablation_results}

## 4. ç»“è®ºä¸å»ºè®®
...
"""
    return report
```

---

## ğŸ“ é…ç½®å‚æ•°

```yaml
evaluation:
  metrics: ["RMSE", "MAE", "MAPE", "CRPS"]
  
interpretability:
  attention_visualization: true
  gradient_analysis: true
  counterfactual_analysis: true
  ablation_study: true
  
counterfactual:
  perturbations: [0.1, 0.25, 0.5, 0.75]
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å¤šæ–¹æ³•éªŒè¯**: ç”¨æ¢¯åº¦åˆ†æéªŒè¯æ³¨æ„åŠ›æƒé‡å¯é æ€§
2. **ç»Ÿè®¡æ£€éªŒ**: ä½¿ç”¨DMæ£€éªŒç¡®è®¤æ€§èƒ½å·®å¼‚æ˜¾è‘—æ€§
3. **å› æœè§£é‡Š**: åäº‹å®åˆ†æä»…æä¾›ç›¸å…³æ€§ï¼Œéå› æœå…³ç³»
4. **å¯è§†åŒ–**: æ³¨æ„åŠ›çƒ­å›¾åº”åŒ…å«ç½®ä¿¡åŒºé—´
