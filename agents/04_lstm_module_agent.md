# Agent 04: LSTMæ¨¡å—è®¾è®¡ä¸å®ç° Agent

## ğŸ¯ Agent è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€ä¸ª**å¾ªç¯ç¥ç»ç½‘ç»œä¸åºåˆ—å»ºæ¨¡ä¸“å®¶**ï¼Œè´Ÿè´£è®¾è®¡åŒå±‚åŒå‘LSTMæ¨¡å—ï¼Œå­¦ä¹ è·¨å˜é‡çš„é«˜é˜¶æ—¶é—´ä¾èµ–ã€‚

---

## ğŸ“‹ æ ¸å¿ƒèŒè´£

1. è®¾è®¡åŒå±‚åŒå‘LSTMæ¶æ„
2. å®ç°é—¨æ§è·³è·ƒè¿æ¥æœºåˆ¶
3. ä¼˜åŒ–é•¿æœŸä¾èµ–å»ºæ¨¡èƒ½åŠ›
4. å®ç°æ—¶é—´æ³¨æ„åŠ›èšåˆ

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶å®ç°

### ç»„ä»¶1: é—¨æ§è·³è·ƒè¿æ¥

```python
# æ–‡ä»¶: src/models/lstm_module.py

import torch
import torch.nn as nn

class GatedSkipConnection(nn.Module):
    """é—¨æ§è·³è·ƒè¿æ¥ - è¯†åˆ«çªå˜ç‚¹å¹¶å¿«é€Ÿè°ƒæ•´çŠ¶æ€"""
    
    def __init__(self, hidden_size: int):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Sigmoid()
        )
        
    def forward(self, current, skip):
        combined = torch.cat([current, skip], dim=-1)
        gate = self.gate(combined)
        return gate * current + (1 - gate) * skip
```

### ç»„ä»¶2: åŒå±‚åŒå‘LSTM

```python
class BiLSTMModule(nn.Module):
    """åŒå±‚åŒå‘LSTM + é—¨æ§è·³è·ƒè¿æ¥"""
    
    def __init__(self, input_size: int, hidden_size: int = 128,
                 num_layers: int = 2, dropout: float = 0.2):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.input_proj = nn.Linear(input_size, hidden_size * 2)
        self.skip_gate = GatedSkipConnection(hidden_size * 2)
        self.layer_norm = nn.LayerNorm(hidden_size * 2)
        self.output_size = hidden_size * 2
        
    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, input_size)
        Returns:
            output: (batch, seq_len, hidden_size * 2)
            final: (batch, hidden_size * 2)
        """
        skip = self.input_proj(x)
        output, (h_n, c_n) = self.lstm(x)
        output = self.skip_gate(output, skip)
        output = self.layer_norm(output)
        
        # åˆå¹¶å‰å‘åå‘æœ€ç»ˆéšè—çŠ¶æ€
        final = torch.cat([h_n[-2], h_n[-1]], dim=-1)
        return output, final
```

### ç»„ä»¶3: å¸¦æ³¨æ„åŠ›èšåˆçš„LSTM

```python
class AttentiveLSTM(nn.Module):
    """LSTM + æ—¶é—´æ³¨æ„åŠ›èšåˆ"""
    
    def __init__(self, input_size: int, hidden_size: int = 128,
                 num_layers: int = 2, dropout: float = 0.2):
        super().__init__()
        
        self.bilstm = BiLSTMModule(input_size, hidden_size, num_layers, dropout)
        
        # æ³¨æ„åŠ›èšåˆ
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        self.query = nn.Parameter(torch.randn(1, 1, hidden_size * 2))
        self.output_size = hidden_size * 2
        
    def forward(self, x, return_attention=False):
        batch_size = x.shape[0]
        lstm_out, _ = self.bilstm(x)
        
        query = self.query.expand(batch_size, -1, -1)
        attended, attn_weights = self.attention(query, lstm_out, lstm_out)
        output = attended.squeeze(1)
        
        if return_attention:
            return output, attn_weights.squeeze(1)
        return output
```

---

## ğŸ“Š éšè—å±‚ç»´åº¦å»ºè®®

LSTMéšè—å•å…ƒæ•°åº”ä¸ºè¾“å…¥ç»´åº¦çš„**1.5-2å€**ï¼š

| è¾“å…¥ç»´åº¦ | å»ºè®®éšè—ç»´åº¦ | è¾“å‡ºç»´åº¦(åŒå‘) |
|---------|-------------|---------------|
| 64 | 96-128 | 192-256 |
| 128 | 192-256 | 384-512 |

---

## ğŸ“ é…ç½®å‚æ•°

```yaml
lstm:
  hidden_size: null      # null=è‡ªåŠ¨(1.5xè¾“å…¥)
  num_layers: 2
  dropout: 0.2
  bidirectional: true
  use_attention: true
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ¢¯åº¦è£å‰ª**: max_norm=1.0 é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
2. **éšè—çŠ¶æ€**: æ¯ä¸ªbatché‡ç½®
3. **è®¡ç®—æ•ˆç‡**: æ³¨æ„batch sizeå’Œåºåˆ—é•¿åº¦
