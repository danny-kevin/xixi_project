# Agent 02: M-TCNæ¨¡å—è®¾è®¡ä¸å®ç° Agent

## ğŸ¯ Agent è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€ä¸ª**æ—¶é—´å·ç§¯ç½‘ç»œ(TCN)æ¶æ„ä¸“å®¶**ï¼Œä¸“é—¨è´Ÿè´£è®¾è®¡å’Œå®ç°M-TCNï¼ˆå¤šå¤´åˆ†ç¦»å¼æ—¶é—´å·ç§¯ç½‘ç»œï¼‰æ¨¡å—ï¼Œç”¨äºæå–å¤šå˜é‡æ—¶é—´åºåˆ—çš„å˜é‡ç‰¹å¼‚æ€§ç‰¹å¾ã€‚

---

## ğŸ“‹ æ ¸å¿ƒèŒè´£

1. è®¾è®¡å› æœå·ç§¯å±‚å’Œæ‰©å¼ å·ç§¯å±‚
2. å®ç°æ®‹å·®å—ç»“æ„
3. æ„å»ºå¤šå¤´å¹¶è¡ŒTCNå­ç½‘ç»œ
4. ä¼˜åŒ–æ„Ÿå—é‡è¦†ç›–èŒƒå›´
5. å®ç°å±‚æ¬¡åŒ–æ‰©å¼ å·ç§¯ç­–ç•¥

---

## ğŸ—ï¸ M-TCNæ¶æ„è¯¦è§£

### æ•´ä½“ç»“æ„
```
M-TCNæ¨¡å—
â”œâ”€â”€ å˜é‡1 â†’ TCNå­ç½‘ç»œ1 (ç‹¬ç«‹å¤„ç†)
â”œâ”€â”€ å˜é‡2 â†’ TCNå­ç½‘ç»œ2 (ç‹¬ç«‹å¤„ç†)
â”œâ”€â”€ å˜é‡3 â†’ TCNå­ç½‘ç»œ3 (ç‹¬ç«‹å¤„ç†)
â”œâ”€â”€ ...
â””â”€â”€ å˜é‡N â†’ TCNå­ç½‘ç»œN (ç‹¬ç«‹å¤„ç†)
          â†“
    ç‰¹å¾æ‹¼æ¥ (Concatenation)
          â†“
    è¾“å‡º: (batch, time_steps, N Ã— feature_dim)
```

### å•ä¸ªTCNå­ç½‘ç»œç»“æ„
```
è¾“å…¥: (batch, time_steps, 1)
        â†“
æ®‹å·®å—1 (æ‰©å¼ ç³»æ•° d=1)  â†’ æ„Ÿå—é‡: 1-3å¤©
        â†“
æ®‹å·®å—2 (æ‰©å¼ ç³»æ•° d=2)  â†’ æ„Ÿå—é‡: 4-7å¤©
        â†“
æ®‹å·®å—3 (æ‰©å¼ ç³»æ•° d=4)  â†’ æ„Ÿå—é‡: 8-14å¤©
        â†“
æ®‹å·®å—4 (æ‰©å¼ ç³»æ•° d=8)  â†’ æ„Ÿå—é‡: 15-28å¤©
        â†“
è¾“å‡º: (batch, time_steps, feature_dim)
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶å®ç°

### ç»„ä»¶1: å› æœå·ç§¯å±‚

```python
# æ–‡ä»¶: src/models/tcn.py

import torch
import torch.nn as nn
from torch.nn.utils import weight_norm

class CausalConv1d(nn.Module):
    """å› æœå·ç§¯å±‚ - ç¡®ä¿ä¸ä½¿ç”¨æœªæ¥ä¿¡æ¯"""
    
    def __init__(self, in_channels: int, out_channels: int, 
                 kernel_size: int, dilation: int = 1):
        """
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°
            kernel_size: å·ç§¯æ ¸å¤§å°
            dilation: æ‰©å¼ ç³»æ•°
        """
        super(CausalConv1d, self).__init__()
        
        # è®¡ç®—å› æœå¡«å……ï¼šç¡®ä¿è¾“å‡ºåªä¾èµ–äºè¿‡å»å’Œå½“å‰çš„è¾“å…¥
        self.padding = (kernel_size - 1) * dilation
        
        self.conv = weight_norm(nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            padding=self.padding,
            dilation=dilation
        ))
        
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, channels, time_steps)
        Returns:
            è¾“å‡ºå¼ é‡, å½¢çŠ¶ (batch, out_channels, time_steps)
        """
        out = self.conv(x)
        # ç§»é™¤å³ä¾§å¡«å……ï¼Œç¡®ä¿å› æœæ€§
        if self.padding > 0:
            out = out[:, :, :-self.padding]
        return out
```

### ç»„ä»¶2: æ®‹å·®å—

```python
class ResidualBlock(nn.Module):
    """TCNæ®‹å·®å— - åŒ…å«ä¸¤å±‚å› æœå·ç§¯å’Œæ®‹å·®è¿æ¥"""
    
    def __init__(self, n_inputs: int, n_outputs: int, 
                 kernel_size: int, dilation: int, dropout: float = 0.2):
        """
        Args:
            n_inputs: è¾“å…¥é€šé“æ•°
            n_outputs: è¾“å‡ºé€šé“æ•°
            kernel_size: å·ç§¯æ ¸å¤§å°
            dilation: æ‰©å¼ ç³»æ•°
            dropout: Dropoutæ¦‚ç‡
        """
        super(ResidualBlock, self).__init__()
        
        # ç¬¬ä¸€å±‚å› æœå·ç§¯
        self.conv1 = CausalConv1d(n_inputs, n_outputs, kernel_size, dilation)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        
        # ç¬¬äºŒå±‚å› æœå·ç§¯
        self.conv2 = CausalConv1d(n_outputs, n_outputs, kernel_size, dilation)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        
        # æ®‹å·®è¿æ¥ï¼šå¦‚æœè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒï¼Œéœ€è¦1x1å·ç§¯è°ƒæ•´
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        
        self.relu = nn.ReLU()
        
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, n_inputs, time_steps)
        Returns:
            è¾“å‡ºå¼ é‡, å½¢çŠ¶ (batch, n_outputs, time_steps)
        """
        # ä¸»è·¯å¾„
        out = self.conv1(x)
        out = self.relu1(out)
        out = self.dropout1(out)
        
        out = self.conv2(out)
        out = self.relu2(out)
        out = self.dropout2(out)
        
        # æ®‹å·®è¿æ¥
        res = x if self.downsample is None else self.downsample(x)
        
        return self.relu(out + res)
```

### ç»„ä»¶3: å•å˜é‡TCNå­ç½‘ç»œ

```python
class TCNSubNetwork(nn.Module):
    """å•å˜é‡TCNå­ç½‘ç»œ - æå–å•ä¸ªå˜é‡çš„æ—¶é—´ç‰¹å¾"""
    
    def __init__(self, input_size: int = 1, 
                 num_channels: list = [32, 32, 32, 32],
                 kernel_size: int = 3, 
                 dropout: float = 0.2):
        """
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆå•å˜é‡ä¸º1ï¼‰
            num_channels: æ¯ä¸ªæ®‹å·®å—çš„è¾“å‡ºé€šé“æ•°åˆ—è¡¨
            kernel_size: å·ç§¯æ ¸å¤§å°
            dropout: Dropoutæ¦‚ç‡
        """
        super(TCNSubNetwork, self).__init__()
        
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            # æ‰©å¼ ç³»æ•°æŒ‰2çš„æŒ‡æ•°å¢é•¿: 1, 2, 4, 8, ...
            dilation = 2 ** i
            
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            
            layers.append(ResidualBlock(
                n_inputs=in_channels,
                n_outputs=out_channels,
                kernel_size=kernel_size,
                dilation=dilation,
                dropout=dropout
            ))
        
        self.network = nn.Sequential(*layers)
        self.receptive_field = self._calculate_receptive_field(
            num_levels, kernel_size
        )
        
    def _calculate_receptive_field(self, num_levels: int, kernel_size: int) -> int:
        """è®¡ç®—æ„Ÿå—é‡å¤§å°"""
        # æ„Ÿå—é‡ = 1 + 2 * (kernel_size - 1) * sum(2^i for i in range(num_levels))
        return 1 + 2 * (kernel_size - 1) * (2 ** num_levels - 1)
    
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, time_steps, 1)
        Returns:
            è¾“å‡ºå¼ é‡, å½¢çŠ¶ (batch, time_steps, num_channels[-1])
        """
        # è°ƒæ•´ç»´åº¦: (batch, time, channels) -> (batch, channels, time)
        x = x.transpose(1, 2)
        out = self.network(x)
        # è¿˜åŸç»´åº¦: (batch, channels, time) -> (batch, time, channels)
        return out.transpose(1, 2)
```

### ç»„ä»¶4: å®Œæ•´M-TCNæ¨¡å—

```python
class MTCN(nn.Module):
    """å¤šå¤´åˆ†ç¦»å¼æ—¶é—´å·ç§¯ç½‘ç»œ (M-TCN)
    
    æ¯ä¸ªè¾“å…¥å˜é‡é€šè¿‡ç‹¬ç«‹çš„TCNå­ç½‘ç»œå¤„ç†ï¼Œ
    æå–å˜é‡ç‰¹å¼‚æ€§çš„æ—¶é—´æ¨¡å¼ï¼Œç„¶åæ‹¼æ¥è¾“å‡ºã€‚
    """
    
    def __init__(self, num_variables: int,
                 num_channels: list = [32, 32, 32, 32],
                 kernel_size: int = 3,
                 dropout: float = 0.2,
                 share_weights: bool = False):
        """
        Args:
            num_variables: è¾“å…¥å˜é‡æ•°é‡
            num_channels: æ¯ä¸ªæ®‹å·®å—çš„é€šé“æ•°
            kernel_size: å·ç§¯æ ¸å¤§å°
            dropout: Dropoutæ¦‚ç‡
            share_weights: æ˜¯å¦å…±äº«å„å­ç½‘ç»œçš„æƒé‡
        """
        super(MTCN, self).__init__()
        
        self.num_variables = num_variables
        self.share_weights = share_weights
        
        if share_weights:
            # æ‰€æœ‰å˜é‡å…±äº«åŒä¸€ä¸ªTCNç½‘ç»œ
            self.shared_tcn = TCNSubNetwork(
                input_size=1,
                num_channels=num_channels,
                kernel_size=kernel_size,
                dropout=dropout
            )
        else:
            # æ¯ä¸ªå˜é‡æœ‰ç‹¬ç«‹çš„TCNç½‘ç»œ
            self.tcn_list = nn.ModuleList([
                TCNSubNetwork(
                    input_size=1,
                    num_channels=num_channels,
                    kernel_size=kernel_size,
                    dropout=dropout
                ) for _ in range(num_variables)
            ])
        
        # è¾“å‡ºç‰¹å¾ç»´åº¦
        self.output_dim = num_variables * num_channels[-1]
        
        # æ‰“å°æ„Ÿå—é‡ä¿¡æ¯
        sample_tcn = self.shared_tcn if share_weights else self.tcn_list[0]
        print(f"M-TCNæ„Ÿå—é‡: {sample_tcn.receptive_field} æ—¶é—´æ­¥")
        
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, time_steps, num_variables)
        Returns:
            è¾“å‡ºå¼ é‡, å½¢çŠ¶ (batch, time_steps, num_variables * channel_dim)
        """
        batch_size, time_steps, _ = x.shape
        
        outputs = []
        for i in range(self.num_variables):
            # æå–ç¬¬iä¸ªå˜é‡: (batch, time, 1)
            var_input = x[:, :, i:i+1]
            
            if self.share_weights:
                var_output = self.shared_tcn(var_input)
            else:
                var_output = self.tcn_list[i](var_input)
            
            outputs.append(var_output)
        
        # æ‹¼æ¥æ‰€æœ‰å˜é‡çš„è¾“å‡º: (batch, time, num_vars * channels)
        concatenated = torch.cat(outputs, dim=-1)
        
        return concatenated
```

---

## ğŸ“Š å±‚æ¬¡åŒ–æ‰©å¼ å·ç§¯ç­–ç•¥

é’ˆå¯¹ä¼ æŸ“ç—…çš„å¤šå°ºåº¦æ»åæ•ˆåº”ï¼Œè®¾è®¡å±‚æ¬¡åŒ–æ„Ÿå—é‡ï¼š

```python
class HierarchicalMTCN(nn.Module):
    """å±‚æ¬¡åŒ–M-TCN - åˆ†å±‚æ•æ‰ä¸åŒæ—¶é—´å°ºåº¦çš„æ¨¡å¼"""
    
    def __init__(self, num_variables: int):
        super(HierarchicalMTCN, self).__init__()
        
        # çŸ­æœŸæ¨¡å¼æ•æ‰ (1-3å¤©) - æ‰©å¼ ç³»æ•°: [1]
        self.short_term = MTCN(
            num_variables=num_variables,
            num_channels=[16, 16],
            kernel_size=3,
            dropout=0.1
        )
        
        # ä¸­æœŸæ¨¡å¼æ•æ‰ (7-10å¤©) - æ‰©å¼ ç³»æ•°: [1, 2, 4]
        self.medium_term = MTCN(
            num_variables=num_variables,
            num_channels=[32, 32, 32],
            kernel_size=3,
            dropout=0.15
        )
        
        # é•¿æœŸæ¨¡å¼æ•æ‰ (14-21å¤©) - æ‰©å¼ ç³»æ•°: [1, 2, 4, 8]
        self.long_term = MTCN(
            num_variables=num_variables,
            num_channels=[32, 32, 32, 32],
            kernel_size=3,
            dropout=0.2
        )
        
        # èåˆå±‚
        total_features = (self.short_term.output_dim + 
                         self.medium_term.output_dim + 
                         self.long_term.output_dim)
        self.fusion = nn.Linear(total_features, 128)
        
    def forward(self, x):
        short = self.short_term(x)
        medium = self.medium_term(x)
        long = self.long_term(x)
        
        # å¤šå°ºåº¦ç‰¹å¾èåˆ
        fused = torch.cat([short, medium, long], dim=-1)
        return self.fusion(fused)
```

---

## ğŸ§® æ„Ÿå—é‡è®¡ç®—å…¬å¼

å¯¹äºæ ‡å‡†TCNï¼š
```
æ„Ÿå—é‡ = 1 + Î£(kernel_size - 1) Ã— dilation_i Ã— 2
       = 1 + 2 Ã— (kernel_size - 1) Ã— (2^num_layers - 1)
```

| å±‚æ•° | æ‰©å¼ ç³»æ•° | æ„Ÿå—é‡(kernel=3) |
|-----|---------|-----------------|
| 1å±‚ | [1] | 5 |
| 2å±‚ | [1,2] | 9 |
| 3å±‚ | [1,2,4] | 17 |
| 4å±‚ | [1,2,4,8] | 33 |
| 5å±‚ | [1,2,4,8,16] | 65 |

**æ¨èé…ç½®**: 4å±‚æ®‹å·®å—ï¼Œæ„Ÿå—é‡33å¤©ï¼Œå¯è¦†ç›–14-21å¤©çš„æ»åå‘¨æœŸã€‚

---

## ğŸ“ é…ç½®å‚æ•°è¯´æ˜

```yaml
# configs/mtcn_config.yaml
mtcn:
  num_variables: 10           # è¾“å…¥å˜é‡æ•°é‡
  num_channels: [32, 32, 32, 32]  # å„æ®‹å·®å—é€šé“æ•°
  kernel_size: 3              # å·ç§¯æ ¸å¤§å°
  dropout: 0.2                # Dropoutæ¦‚ç‡
  share_weights: false        # æ˜¯å¦å…±äº«æƒé‡
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å› æœæ€§ä¿è¯**: ç¡®ä¿æ‰€æœ‰å·ç§¯æ“ä½œåªä½¿ç”¨å†å²ä¿¡æ¯
2. **æ„Ÿå—é‡è¦†ç›–**: æ„Ÿå—é‡å¿…é¡»è¦†ç›–æœ€é•¿æ»åå‘¨æœŸï¼ˆ21å¤©ä»¥ä¸Šï¼‰
3. **æ¢¯åº¦ç¨³å®š**: ä½¿ç”¨æƒé‡å½’ä¸€åŒ–é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
4. **å†…å­˜ä¼˜åŒ–**: å¯¹äºå¤§é‡å˜é‡ï¼Œè€ƒè™‘åˆ†æ‰¹å¤„ç†æˆ–å…±äº«æƒé‡
5. **è¾“å…¥éªŒè¯**: ç¡®ä¿è¾“å…¥åºåˆ—é•¿åº¦å¤§äºæ„Ÿå—é‡
