# Agent 05: æ¨¡å‹æ•´åˆä¸è®­ç»ƒ Agent

## ğŸ¯ Agent è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€ä¸ª**æ·±åº¦å­¦ä¹ è®­ç»ƒä¸ä¼˜åŒ–ä¸“å®¶**ï¼Œè´Ÿè´£å°†M-TCNã€æ³¨æ„åŠ›å±‚ã€LSTMæ•´åˆä¸ºå®Œæ•´æ¨¡å‹ï¼Œå¹¶å®ç°å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚

---

## ğŸ“‹ æ ¸å¿ƒèŒè´£

1. æ•´åˆå„å­æ¨¡å—ä¸ºå®Œæ•´æ··åˆæ¨¡å‹
2. å®ç°å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆé¢„è®­ç»ƒ+å¾®è°ƒï¼‰
3. è®¾è®¡æŸå¤±å‡½æ•°ä¸æ­£åˆ™åŒ–
4. é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
5. å®ç°æ—¶åºäº¤å‰éªŒè¯

---

## ğŸ—ï¸ å®Œæ•´æ¨¡å‹æ¶æ„

```python
# æ–‡ä»¶: src/models/hybrid_model.py

import torch
import torch.nn as nn
from .mtcn import MTCN
from .attention import SpatioTemporalAttention
from .lstm_module import AttentiveLSTM

class MTCNLSTMHybrid(nn.Module):
    """æ³¨æ„åŠ›å¢å¼ºM-TCN-LSTMæ··åˆæ¨¡å‹"""
    
    def __init__(self, num_variables: int, tcn_channels: list = [32,32,32,32],
                 lstm_hidden: int = 128, output_steps: int = 7, dropout: float = 0.2):
        super().__init__()
        
        # M-TCNæ¨¡å—
        self.mtcn = MTCN(
            num_variables=num_variables,
            num_channels=tcn_channels,
            kernel_size=3,
            dropout=dropout
        )
        
        mtcn_out_dim = num_variables * tcn_channels[-1]
        
        # æ³¨æ„åŠ›å¢å¼ºå±‚
        self.attention = SpatioTemporalAttention(
            num_variables=num_variables,
            feature_dim=tcn_channels[-1],
            num_heads=4,
            dropout=dropout,
            stochastic_dropout=0.1
        )
        
        # LSTMæ¨¡å—
        self.lstm = AttentiveLSTM(
            input_size=mtcn_out_dim,
            hidden_size=lstm_hidden,
            num_layers=2,
            dropout=dropout
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(lstm_hidden * 2, lstm_hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(lstm_hidden, output_steps)
        )
        
    def forward(self, x, return_attention=False):
        """
        Args:
            x: (batch, time_steps, num_variables)
        Returns:
            predictions: (batch, output_steps)
        """
        # M-TCNç‰¹å¾æå–
        tcn_out = self.mtcn(x)
        
        # æ³¨æ„åŠ›å¢å¼º
        attn_out, attn_weights = self.attention(tcn_out, return_attention=True)
        
        # LSTMåºåˆ—å»ºæ¨¡
        lstm_out, time_attn = self.lstm(attn_out, return_attention=True)
        
        # é¢„æµ‹è¾“å‡º
        predictions = self.output_layer(lstm_out)
        
        if return_attention:
            return predictions, {'variable': attn_weights, 'temporal': time_attn}
        return predictions
```

---

## ğŸ“Š æŸå¤±å‡½æ•°è®¾è®¡

```python
# æ–‡ä»¶: src/training/loss.py

import torch
import torch.nn as nn

class HybridLoss(nn.Module):
    """æ··åˆæŸå¤±å‡½æ•° = RMSE + æ—¶åºä¸€è‡´æ€§æ­£åˆ™"""
    
    def __init__(self, consistency_weight: float = 0.1):
        super().__init__()
        self.mse = nn.MSELoss()
        self.consistency_weight = consistency_weight
        
    def forward(self, pred, target):
        # RMSEæŸå¤±
        rmse_loss = torch.sqrt(self.mse(pred, target))
        
        # æ—¶åºä¸€è‡´æ€§æ­£åˆ™ï¼ˆæƒ©ç½šéç”Ÿç†æ€§éœ‡è¡ï¼‰
        if pred.shape[1] > 1:
            diff = pred[:, 1:] - pred[:, :-1]
            consistency_loss = torch.mean(diff ** 2)
        else:
            consistency_loss = 0
        
        return rmse_loss + self.consistency_weight * consistency_loss
```

---

## ğŸ”„ å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥

```python
# æ–‡ä»¶: src/training/trainer.py

class MultiStageTrainer:
    """å¤šé˜¶æ®µè®­ç»ƒå™¨"""
    
    def __init__(self, model, config):
        self.model = model
        self.config = config
        
    def pretrain_mtcn(self, train_loader, epochs=10):
        """é˜¶æ®µ1: é¢„è®­ç»ƒM-TCN"""
        # å†»ç»“LSTMï¼Œåªè®­ç»ƒM-TCN
        for param in self.model.lstm.parameters():
            param.requires_grad = False
            
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=0.001
        )
        
        for epoch in range(epochs):
            self._train_epoch(train_loader, optimizer)
            
        # è§£å†»
        for param in self.model.lstm.parameters():
            param.requires_grad = True
            
    def pretrain_lstm(self, train_loader, epochs=10):
        """é˜¶æ®µ2: é¢„è®­ç»ƒLSTM"""
        # å†»ç»“M-TCN
        for param in self.model.mtcn.parameters():
            param.requires_grad = False
            
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=0.001
        )
        
        for epoch in range(epochs):
            self._train_epoch(train_loader, optimizer)
            
        # è§£å†»
        for param in self.model.mtcn.parameters():
            param.requires_grad = True
            
    def finetune(self, train_loader, val_loader, epochs=50):
        """é˜¶æ®µ3: ç«¯åˆ°ç«¯è”åˆå¾®è°ƒ"""
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0001)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs
        )
        
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            train_loss = self._train_epoch(train_loader, optimizer)
            val_loss = self._validate(val_loader)
            scheduler.step()
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self._save_checkpoint()
                
    def _train_epoch(self, loader, optimizer):
        self.model.train()
        total_loss = 0
        criterion = HybridLoss()
        
        for x, y in loader:
            optimizer.zero_grad()
            pred = self.model(x)
            loss = criterion(pred, y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item()
            
        return total_loss / len(loader)
```

---

## ğŸ“ˆ æ—¶åºäº¤å‰éªŒè¯

```python
class TimeSeriesCV:
    """æ—¶åºäº¤å‰éªŒè¯ - é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²"""
    
    def __init__(self, n_splits=5, val_ratio=0.2):
        self.n_splits = n_splits
        self.val_ratio = val_ratio
        
    def split(self, data):
        n = len(data)
        fold_size = n // (self.n_splits + 1)
        
        for i in range(self.n_splits):
            train_end = fold_size * (i + 1)
            val_end = min(train_end + int(fold_size * self.val_ratio), n)
            
            yield {
                'train': (0, train_end),
                'val': (train_end, val_end)
            }
```

---

## ğŸ“ è®­ç»ƒé…ç½®

```yaml
# configs/training_config.yaml
training:
  pretrain_mtcn_epochs: 10
  pretrain_lstm_epochs: 10
  finetune_epochs: 50
  batch_size: 32
  
optimizer:
  type: "AdamW"
  lr: 0.001
  weight_decay: 0.01
  
scheduler:
  type: "CosineAnnealing"
  T_max: 50
  
loss:
  consistency_weight: 0.1
  
gradient:
  clip_norm: 1.0
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å¤šé˜¶æ®µé¡ºåº**: å…ˆM-TCN â†’ å†LSTM â†’ æœ€åè”åˆå¾®è°ƒ
2. **å­¦ä¹ ç‡**: å¾®è°ƒé˜¶æ®µä½¿ç”¨æ›´å°å­¦ä¹ ç‡(1e-4)
3. **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
4. **æ—©åœ**: éªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶åœæ­¢
5. **æ—¶åºéªŒè¯**: ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®
