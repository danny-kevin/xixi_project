# Agent 01: æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç† Agent

## ğŸ¯ Agent è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€ä¸ª**æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†ä¸“å®¶**ï¼Œä¸“é—¨è´Ÿè´£ä¼ æŸ“ç—…é¢„æµ‹æ¨¡å‹æ‰€éœ€çš„å¤šæºå¼‚æ„æ•°æ®çš„æ”¶é›†ã€æ¸…æ´—ã€è½¬æ¢å’Œé¢„å¤„ç†å·¥ä½œã€‚

---

## ğŸ“‹ æ ¸å¿ƒèŒè´£

1. å¤šæºæ•°æ®æ”¶é›†ä¸æ•´åˆ
2. æ•°æ®è´¨é‡æ£€æŸ¥ä¸æ¸…æ´—
3. ç¼ºå¤±å€¼å¤„ç†ä¸å¼‚å¸¸å€¼æ£€æµ‹
4. ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®å˜æ¢
5. æ•°æ®é›†åˆ’åˆ†ä¸DataLoaderæ„å»º

---

## ğŸ“Š æ•°æ®ç±»å‹è¯¦è§£

### 1. ç–«æƒ…æ•°æ®
```python
# å­—æ®µè¯´æ˜
epidemic_features = {
    'new_cases': 'æ¯æ—¥æ–°å¢ç¡®è¯Šç—…ä¾‹æ•°',
    'new_deaths': 'æ¯æ—¥æ–°å¢æ­»äº¡ç—…ä¾‹æ•°', 
    'new_recovered': 'æ¯æ—¥æ–°å¢åº·å¤ç—…ä¾‹æ•°',
    'cumulative_cases': 'ç´¯è®¡ç¡®è¯Šç—…ä¾‹æ•°',
    'cumulative_deaths': 'ç´¯è®¡æ­»äº¡ç—…ä¾‹æ•°',
    'active_cases': 'ç°å­˜ç—…ä¾‹æ•°'
}
```

### 2. äººå£æµåŠ¨æ•°æ®
```python
mobility_features = {
    'intra_city_flow': 'å¸‚å†…äººå£æµåŠ¨æŒ‡æ•°',
    'inter_city_flow': 'åŸé™…äººå£æµåŠ¨æŒ‡æ•°',
    'public_transport': 'å…¬å…±äº¤é€šä½¿ç”¨æŒ‡æ•°',
    'retail_mobility': 'é›¶å”®åœºæ‰€äººæµæŒ‡æ•°',
    'workplace_mobility': 'å·¥ä½œåœºæ‰€äººæµæŒ‡æ•°'
}
```

### 3. ç¯å¢ƒæ•°æ®
```python
environmental_features = {
    'temperature': 'æ—¥å‡æ¸©åº¦(â„ƒ)',
    'humidity': 'ç›¸å¯¹æ¹¿åº¦(%)',
    'uv_index': 'ç´«å¤–çº¿æŒ‡æ•°',
    'precipitation': 'é™æ°´é‡(mm)',
    'wind_speed': 'é£é€Ÿ(m/s)'
}
```

### 4. å¹²é¢„æ”¿ç­–æ•°æ®
```python
policy_features = {
    'lockdown_level': 'å°åŸç­‰çº§(0-4)',
    'social_distance': 'ç¤¾äº¤è·ç¦»è¦æ±‚ç­‰çº§(0-3)',
    'mask_mandate': 'å£ç½©ä»¤(0/1)',
    'vaccination_rate': 'ç–«è‹—æ¥ç§ç‡(%)',
    'testing_rate': 'æ£€æµ‹ç‡(æ¯åƒäºº)'
}
```

---

## ğŸ”§ æ•°æ®é¢„å¤„ç†ä»»åŠ¡

### ä»»åŠ¡1: æ•°æ®åŠ è½½ä¸åˆæ­¥æ£€æŸ¥

```python
# æ–‡ä»¶: src/data/data_loader.py

class EpidemicDataLoader:
    """å¤šæºç–«æƒ…æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.data_sources = {}
    
    def load_epidemic_data(self, path: str) -> pd.DataFrame:
        """åŠ è½½ç–«æƒ…æ ¸å¿ƒæ•°æ®"""
        # TODO: å®ç°æ•°æ®åŠ è½½é€»è¾‘
        pass
    
    def load_mobility_data(self, path: str) -> pd.DataFrame:
        """åŠ è½½äººå£æµåŠ¨æ•°æ®"""
        pass
    
    def load_environmental_data(self, path: str) -> pd.DataFrame:
        """åŠ è½½ç¯å¢ƒæ•°æ®"""
        pass
    
    def load_policy_data(self, path: str) -> pd.DataFrame:
        """åŠ è½½æ”¿ç­–å¹²é¢„æ•°æ®"""
        pass
    
    def merge_all_sources(self) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰æ•°æ®æºï¼Œç»Ÿä¸€æ—¶é—´ç´¢å¼•"""
        pass
```

### ä»»åŠ¡2: ç¼ºå¤±å€¼å¤„ç†

```python
# æ–‡ä»¶: src/data/preprocessor.py

class MissingValueHandler:
    """ç¼ºå¤±å€¼å¤„ç†å™¨ - ä½¿ç”¨æ—¶ç©ºå…‹é‡Œé‡‘æ’å€¼"""
    
    def __init__(self, method='spatiotemporal_kriging'):
        self.method = method
    
    def detect_missing(self, df: pd.DataFrame) -> dict:
        """æ£€æµ‹ç¼ºå¤±å€¼åˆ†å¸ƒ"""
        missing_report = {
            'total_missing': df.isnull().sum().sum(),
            'missing_by_column': df.isnull().sum().to_dict(),
            'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict()
        }
        return missing_report
    
    def temporal_interpolation(self, series: pd.Series) -> pd.Series:
        """æ—¶é—´ç»´åº¦æ’å€¼"""
        return series.interpolate(method='time')
    
    def spatial_kriging(self, df: pd.DataFrame, coords: np.ndarray) -> pd.DataFrame:
        """ç©ºé—´å…‹é‡Œé‡‘æ’å€¼"""
        # TODO: å®ç°ç©ºé—´æ’å€¼
        pass
    
    def spatiotemporal_kriging(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ—¶ç©ºå…‹é‡Œé‡‘æ’å€¼ - ç»¼åˆè€ƒè™‘æ—¶é—´å’Œç©ºé—´ç›¸å…³æ€§"""
        # TODO: å®ç°æ—¶ç©ºè”åˆæ’å€¼
        pass
```

### ä»»åŠ¡3: æ•°æ®å˜æ¢ä¸æ ‡å‡†åŒ–

```python
# æ–‡ä»¶: src/data/preprocessor.py

class DataTransformer:
    """æ•°æ®å˜æ¢å™¨"""
    
    def __init__(self):
        self.scalers = {}
        self.box_cox_lambdas = {}
    
    def box_cox_transform(self, data: np.ndarray, column_name: str) -> np.ndarray:
        """Box-Coxå˜æ¢ - ç¨³å®šæ–¹å·®"""
        from scipy.stats import boxcox
        # ç¡®ä¿æ•°æ®ä¸ºæ­£å€¼
        data_positive = data - data.min() + 1
        transformed, lmbda = boxcox(data_positive)
        self.box_cox_lambdas[column_name] = lmbda
        return transformed
    
    def difference(self, series: pd.Series, periods: int = 1) -> pd.Series:
        """å·®åˆ†å¤„ç† - æ¶ˆé™¤è¶‹åŠ¿é¡¹"""
        return series.diff(periods=periods)
    
    def moving_average_ratio(self, series: pd.Series, window: int = 7) -> pd.Series:
        """ç§»åŠ¨å¹³å‡æ¯”ç‡æ³• - æ¶ˆé™¤å‘¨æœŸæ€§"""
        ma = series.rolling(window=window, center=True).mean()
        return series / ma
    
    def normalize(self, data: np.ndarray, method: str = 'minmax') -> np.ndarray:
        """æ•°æ®æ ‡å‡†åŒ–"""
        if method == 'minmax':
            from sklearn.preprocessing import MinMaxScaler
            scaler = MinMaxScaler()
        elif method == 'standard':
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
        return scaler.fit_transform(data.reshape(-1, 1)).flatten()
```

### ä»»åŠ¡4: å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†

```python
class OutlierHandler:
    """å¼‚å¸¸å€¼å¤„ç†å™¨ - ç»“åˆæµè¡Œç—…å­¦å…ˆéªŒ"""
    
    def __init__(self, method='iqr_epidemiological'):
        self.method = method
    
    def detect_outliers_iqr(self, series: pd.Series, threshold: float = 1.5) -> pd.Series:
        """IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        Q1, Q3 = series.quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        return (series < lower_bound) | (series > upper_bound)
    
    def detect_reporting_delays(self, series: pd.Series) -> pd.Series:
        """æ£€æµ‹æŠ¥å‘Šå»¶è¿Ÿå¯¼è‡´çš„å¼‚å¸¸å³°å€¼"""
        # å‘¨ä¸€é€šå¸¸æœ‰æŠ¥å‘Šå †ç§¯
        # TODO: å®ç°åŸºäºæ˜ŸæœŸçš„å¼‚å¸¸æ£€æµ‹
        pass
    
    def smooth_with_epidemiological_prior(self, series: pd.Series, 
                                          R0_estimate: float = 2.5) -> pd.Series:
        """ç»“åˆæµè¡Œç—…å­¦å…ˆéªŒè¿›è¡Œå¹³æ»‘
        
        åŸºäºåŸºæœ¬å†ç”Ÿæ•°R0ï¼Œç—…ä¾‹å¢é•¿åº”æ»¡è¶³ä¸€å®šçš„ç”Ÿç‰©å­¦çº¦æŸ
        """
        # TODO: å®ç°æµè¡Œç—…å­¦çº¦æŸçš„å¹³æ»‘
        pass
```

### ä»»åŠ¡5: æ—¶åºæ•°æ®é›†æ„å»º

```python
# æ–‡ä»¶: src/data/dataset.py

import torch
from torch.utils.data import Dataset, DataLoader

class EpidemicTimeSeriesDataset(Dataset):
    """ä¼ æŸ“ç—…æ—¶åºæ•°æ®é›†"""
    
    def __init__(self, data: np.ndarray, 
                 input_window: int = 21,      # è¾“å…¥çª—å£: 21å¤©
                 output_window: int = 7,       # é¢„æµ‹çª—å£: 7å¤©
                 target_column: int = 0):      # ç›®æ ‡åˆ—ç´¢å¼•
        """
        Args:
            data: å½¢çŠ¶ä¸º (æ—¶é—´æ­¥, å˜é‡æ•°) çš„æ•°ç»„
            input_window: è¾“å…¥æ—¶é—´çª—å£é•¿åº¦
            output_window: é¢„æµ‹æ—¶é—´çª—å£é•¿åº¦
            target_column: é¢„æµ‹ç›®æ ‡åˆ—çš„ç´¢å¼•
        """
        self.data = torch.FloatTensor(data)
        self.input_window = input_window
        self.output_window = output_window
        self.target_column = target_column
        
    def __len__(self):
        return len(self.data) - self.input_window - self.output_window + 1
    
    def __getitem__(self, idx):
        # è¾“å…¥åºåˆ—: (input_window, num_features)
        x = self.data[idx:idx + self.input_window]
        # è¾“å‡ºåºåˆ—: (output_window,) - ä»…ç›®æ ‡å˜é‡
        y = self.data[idx + self.input_window:idx + self.input_window + self.output_window, 
                      self.target_column]
        return x, y


class TimeSeriesSplitter:
    """æ—¶åºäº¤å‰éªŒè¯åˆ†å‰²å™¨ - ä¸¥æ ¼æ—¶åºï¼Œé˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²"""
    
    def __init__(self, n_splits: int = 5, test_size: float = 0.2):
        self.n_splits = n_splits
        self.test_size = test_size
    
    def split(self, data: np.ndarray):
        """ç”Ÿæˆæ—¶åºäº¤å‰éªŒè¯çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•ç´¢å¼•"""
        n_samples = len(data)
        test_size = int(n_samples * self.test_size)
        
        for i in range(self.n_splits):
            # æµ‹è¯•é›†å§‹ç»ˆåœ¨æœ€å
            test_start = n_samples - test_size
            test_end = n_samples
            
            # éªŒè¯é›†åœ¨æµ‹è¯•é›†ä¹‹å‰
            val_size = int((n_samples - test_size) * 0.2)
            val_start = test_start - val_size
            val_end = test_start
            
            # è®­ç»ƒé›†åœ¨éªŒè¯é›†ä¹‹å‰
            train_end = val_start
            
            yield {
                'train': (0, train_end),
                'val': (val_start, val_end),
                'test': (test_start, test_end)
            }

### ä»»åŠ¡6ï¼šä»»åŠ¡æ€»ç»“
å®Œæˆä»»åŠ¡ä¹‹åè¯·åœ¨Docs/completion_summary.mdä¸­è®°å½•ä»»åŠ¡å®Œæˆæƒ…å†µ

```

---

## ğŸ“ˆ æ•°æ®è´¨é‡æ£€æŸ¥æ¸…å•

- [ ] æ—¶é—´åºåˆ—è¿ç»­æ€§æ£€æŸ¥ï¼ˆæ— ç¼ºå¤±æ—¥æœŸï¼‰
- [ ] æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
- [ ] æ•°å€¼èŒƒå›´åˆç†æ€§æ£€æŸ¥
- [ ] å¤šæ•°æ®æºæ—¶é—´å¯¹é½éªŒè¯
- [ ] å¼‚å¸¸å€¼æ ‡è®°ä¸è®°å½•
- [ ] ç¼ºå¤±å€¼å¤„ç†è®°å½•
- [ ] æ•°æ®å˜æ¢å¯é€†æ€§éªŒè¯

---

## ğŸ“Š è¾“å‡ºè§„èŒƒ

### é¢„å¤„ç†åæ•°æ®æ ¼å¼
```python
processed_data = {
    'features': np.ndarray,        # å½¢çŠ¶: (æ—¶é—´æ­¥, å˜é‡æ•°)
    'target': np.ndarray,          # å½¢çŠ¶: (æ—¶é—´æ­¥,)
    'timestamps': pd.DatetimeIndex, # æ—¶é—´ç´¢å¼•
    'feature_names': List[str],    # ç‰¹å¾åç§°åˆ—è¡¨
    'scalers': Dict,               # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºåå˜æ¢ï¼‰
    'metadata': Dict               # å…ƒæ•°æ®ä¿¡æ¯
}
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ—¶åºå› æœæ€§**: é¢„å¤„ç†æ—¶ä¸¥ç¦ä½¿ç”¨æœªæ¥æ•°æ®
2. **å¯é€†æ€§**: æ‰€æœ‰å˜æ¢éœ€ä¿å­˜å‚æ•°ï¼Œæ”¯æŒé¢„æµ‹ç»“æœçš„åå˜æ¢
3. **ä¸€è‡´æ€§**: è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†ç®¡é“
4. **æ–‡æ¡£åŒ–**: è®°å½•æ‰€æœ‰é¢„å¤„ç†æ­¥éª¤å’Œå‚æ•°é€‰æ‹©ç†ç”±
