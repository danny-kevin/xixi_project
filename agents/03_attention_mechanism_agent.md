# Agent 03: æ³¨æ„åŠ›æœºåˆ¶å¢å¼º Agent

## ğŸ¯ Agent è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€ä¸ª**æ³¨æ„åŠ›æœºåˆ¶ä¸ç‰¹å¾é€‰æ‹©ä¸“å®¶**ï¼Œä¸“é—¨è´Ÿè´£è®¾è®¡å’Œå®ç°å˜é‡é—´æ³¨æ„åŠ›æ¨¡å—åŠéšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–ï¼Œç”¨äºåŠ¨æ€æ•æ‰å¤šå˜é‡åœ¨ä¸åŒæ—¶ç©ºä¸Šä¸‹æ–‡ä¸­çš„é‡è¦æ€§å˜åŒ–ã€‚

---

## ğŸ“‹ æ ¸å¿ƒèŒè´£

1. å®ç°å˜é‡é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶
2. è®¾è®¡éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–ç­–ç•¥
3. æ„å»ºæ—¶é—´ç»´åº¦æ³¨æ„åŠ›æ¨¡å—
4. å®ç°å¤šå¤´æ³¨æ„åŠ›å˜ä½“
5. å¼€å‘æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–å·¥å…·

---

## ğŸ—ï¸ æ³¨æ„åŠ›æœºåˆ¶æ¶æ„

### åœ¨æ¨¡å‹ä¸­çš„ä½ç½®
```
M-TCNè¾“å‡º (batch, time, NÃ—features)
            â†“
      Flatten/Reshape
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å˜é‡é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ â”‚  â† æœ¬Agentè´Ÿè´£
    â”‚ + éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ– â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    æ³¨æ„åŠ›åŠ æƒç‰¹å¾ (batch, time, NÃ—features)
            â†“
        LSTMæ¨¡å—
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶å®ç°

### ç»„ä»¶1: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

```python
# æ–‡ä»¶: src/models/attention.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ScaledDotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
    
    Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
    """
    
    def __init__(self, d_model: int, dropout: float = 0.1):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super(ScaledDotProductAttention, self).__init__()
        self.scale = math.sqrt(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key, value, mask=None, return_attention=False):
        """
        Args:
            query: (batch, seq_len, d_model) æˆ– (batch, heads, seq_len, d_k)
            key: åŒä¸Š
            value: åŒä¸Š
            mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            context: æ³¨æ„åŠ›åŠ æƒåçš„è¾“å‡º
            attention_weights: (å¯é€‰) æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
        """
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale
        
        # åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        
        # åº”ç”¨Dropout
        attention_weights = self.dropout(attention_weights)
        
        # è®¡ç®—åŠ æƒè¾“å‡º
        context = torch.matmul(attention_weights, value)
        
        if return_attention:
            return context, attention_weights
        return context
```

### ç»„ä»¶2: å˜é‡é—´è‡ªæ³¨æ„åŠ›æ¨¡å—

```python
class VariableAttention(nn.Module):
    """å˜é‡é—´è‡ªæ³¨æ„åŠ›æ¨¡å—
    
    ç”¨äºåŠ¨æ€å­¦ä¹ ä¸åŒè¾“å…¥å˜é‡å¯¹é¢„æµ‹ä»»åŠ¡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œ
    ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´ç‰¹å¾èåˆç­–ç•¥ã€‚
    """
    
    def __init__(self, num_variables: int, 
                 feature_dim: int,
                 num_heads: int = 4,
                 dropout: float = 0.1,
                 stochastic_dropout: float = 0.1):
        """
        Args:
            num_variables: è¾“å…¥å˜é‡æ•°é‡
            feature_dim: æ¯ä¸ªå˜é‡çš„ç‰¹å¾ç»´åº¦
            num_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            dropout: æ ‡å‡†Dropoutæ¦‚ç‡
            stochastic_dropout: éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–çš„ä¸¢å¼ƒæ¦‚ç‡
        """
        super(VariableAttention, self).__init__()
        
        self.num_variables = num_variables
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        self.d_k = feature_dim // num_heads
        
        # Q, K, V çº¿æ€§å˜æ¢
        self.W_q = nn.Linear(feature_dim, feature_dim)
        self.W_k = nn.Linear(feature_dim, feature_dim)
        self.W_v = nn.Linear(feature_dim, feature_dim)
        
        # è¾“å‡ºæŠ•å½±
        self.W_o = nn.Linear(feature_dim, feature_dim)
        
        # æ³¨æ„åŠ›è®¡ç®—
        self.attention = ScaledDotProductAttention(self.d_k, dropout)
        
        # éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–çš„ä¸¢å¼ƒæ¦‚ç‡
        self.stochastic_dropout = stochastic_dropout
        
        # Layer Normalization
        self.layer_norm = nn.LayerNorm(feature_dim)
        
    def _split_heads(self, x, batch_size, num_vars):
        """å°†ç‰¹å¾åˆ†å‰²ä¸ºå¤šå¤´"""
        # (batch, num_vars, features) -> (batch, heads, num_vars, d_k)
        x = x.view(batch_size, num_vars, self.num_heads, self.d_k)
        return x.transpose(1, 2)
    
    def _merge_heads(self, x, batch_size, num_vars):
        """åˆå¹¶å¤šå¤´"""
        # (batch, heads, num_vars, d_k) -> (batch, num_vars, features)
        x = x.transpose(1, 2).contiguous()
        return x.view(batch_size, num_vars, self.feature_dim)
    
    def _stochastic_attention_regularization(self, attention_weights):
        """éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–
        
        åœ¨è®­ç»ƒæœŸé—´éšæœºä¸¢å¼ƒéƒ¨åˆ†æ³¨æ„åŠ›æƒé‡ï¼Œ
        è¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´ç¨³å¥çš„ç‰¹å¾é‡è¦æ€§è¡¨ç¤ºã€‚
        """
        if self.training and self.stochastic_dropout > 0:
            # ç”Ÿæˆéšæœºæ©ç 
            mask = torch.rand_like(attention_weights) > self.stochastic_dropout
            mask = mask.float()
            
            # åº”ç”¨æ©ç å¹¶é‡æ–°å½’ä¸€åŒ–
            masked_weights = attention_weights * mask
            # é¿å…é™¤ä»¥é›¶
            sum_weights = masked_weights.sum(dim=-1, keepdim=True) + 1e-9
            attention_weights = masked_weights / sum_weights
            
        return attention_weights
    
    def forward(self, x, return_attention=False):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, time_steps, num_variables * feature_dim)
               æˆ– (batch, num_variables, feature_dim)
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            output: æ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾
            attention_weights: (å¯é€‰) å˜é‡é—´æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
        """
        batch_size = x.shape[0]
        
        # å¦‚æœè¾“å…¥æ˜¯æ—¶åºæ•°æ®ï¼Œéœ€è¦reshape
        if len(x.shape) == 3 and x.shape[-1] == self.num_variables * self.feature_dim:
            # (batch, time, num_vars * features) -> (batch * time, num_vars, features)
            time_steps = x.shape[1]
            x = x.view(batch_size * time_steps, self.num_variables, self.feature_dim)
            reshape_back = True
        else:
            time_steps = 1
            reshape_back = False
        
        current_batch = x.shape[0]
        
        # æ®‹å·®è¿æ¥
        residual = x
        
        # è®¡ç®— Q, K, V
        Q = self.W_q(x)  # (batch, num_vars, features)
        K = self.W_k(x)
        V = self.W_v(x)
        
        # åˆ†å‰²å¤šå¤´
        Q = self._split_heads(Q, current_batch, self.num_variables)
        K = self._split_heads(K, current_batch, self.num_variables)
        V = self._split_heads(V, current_batch, self.num_variables)
        
        # è®¡ç®—æ³¨æ„åŠ›
        context, attention_weights = self.attention(
            Q, K, V, return_attention=True
        )
        
        # éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–
        attention_weights = self._stochastic_attention_regularization(attention_weights)
        
        # é‡æ–°è®¡ç®—åŠ æƒè¾“å‡ºï¼ˆä½¿ç”¨æ­£åˆ™åŒ–åçš„æƒé‡ï¼‰
        context = torch.matmul(attention_weights, V)
        
        # åˆå¹¶å¤šå¤´
        context = self._merge_heads(context, current_batch, self.num_variables)
        
        # è¾“å‡ºæŠ•å½±
        output = self.W_o(context)
        
        # æ®‹å·®è¿æ¥ + Layer Norm
        output = self.layer_norm(output + residual)
        
        # æ¢å¤æ—¶åºç»´åº¦
        if reshape_back:
            output = output.view(batch_size, time_steps, -1)
            # å¹³å‡æ³¨æ„åŠ›æƒé‡ï¼ˆè·¨æ—¶é—´æ­¥ï¼‰
            attention_weights = attention_weights.view(
                batch_size, time_steps, self.num_heads, 
                self.num_variables, self.num_variables
            ).mean(dim=1)
        
        if return_attention:
            return output, attention_weights
        return output
```

### ç»„ä»¶3: æ—¶é—´æ³¨æ„åŠ›æ¨¡å—

```python
class TemporalAttention(nn.Module):
    """æ—¶é—´ç»´åº¦æ³¨æ„åŠ›æ¨¡å—
    
    æ•æ‰åŒä¸€å˜é‡åœ¨ä¸åŒæ—¶é—´ç‚¹çš„é‡è¦æ€§å˜åŒ–ï¼Œ
    å¼ºåŒ–å¯¹å…³é”®æ—¶é—´ç‚¹ï¼ˆå¦‚ç–«æƒ…è½¬æŠ˜ç‚¹ï¼‰çš„å…³æ³¨ã€‚
    """
    
    def __init__(self, hidden_dim: int, 
                 num_heads: int = 4,
                 dropout: float = 0.1):
        """
        Args:
            hidden_dim: éšè—å±‚ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¦‚ç‡
        """
        super(TemporalAttention, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.d_k = hidden_dim // num_heads
        
        self.W_q = nn.Linear(hidden_dim, hidden_dim)
        self.W_k = nn.Linear(hidden_dim, hidden_dim)
        self.W_v = nn.Linear(hidden_dim, hidden_dim)
        self.W_o = nn.Linear(hidden_dim, hidden_dim)
        
        self.attention = ScaledDotProductAttention(self.d_k, dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, x, causal_mask=True, return_attention=False):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, time_steps, hidden_dim)
            causal_mask: æ˜¯å¦ä½¿ç”¨å› æœæ©ç ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            output: æ—¶é—´æ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾
            attention_weights: (å¯é€‰) æ—¶é—´æ³¨æ„åŠ›æƒé‡
        """
        batch_size, time_steps, _ = x.shape
        residual = x
        
        # ç”Ÿæˆå› æœæ©ç 
        mask = None
        if causal_mask:
            mask = torch.tril(torch.ones(time_steps, time_steps, device=x.device))
            mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, T, T)
        
        # è®¡ç®— Q, K, V
        Q = self.W_q(x).view(batch_size, time_steps, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, time_steps, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, time_steps, self.num_heads, self.d_k).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        context, attention_weights = self.attention(Q, K, V, mask, return_attention=True)
        
        # åˆå¹¶å¤šå¤´
        context = context.transpose(1, 2).contiguous().view(batch_size, time_steps, self.hidden_dim)
        
        # è¾“å‡ºæŠ•å½±
        output = self.W_o(context)
        
        # æ®‹å·® + Layer Norm
        output = self.layer_norm(output + residual)
        
        if return_attention:
            return output, attention_weights
        return output
```

### ç»„ä»¶4: æ—¶ç©ºåŠ¨æ€æ³¨æ„åŠ›æ¨¡å—

```python
class SpatioTemporalAttention(nn.Module):
    """æ—¶ç©ºåŠ¨æ€æ³¨æ„åŠ›æ¨¡å—
    
    ä»æ—¶é—´å’Œå˜é‡ä¸¤ä¸ªç»´åº¦è‡ªé€‚åº”è°ƒæ•´ç‰¹å¾é‡è¦æ€§ï¼š
    - æ—¶é—´ç»´åº¦ï¼šæ•æ‰åŒä¸€å˜é‡åœ¨ä¸åŒç–«æƒ…é˜¶æ®µçš„é‡è¦æ€§å˜åŒ–
    - å˜é‡ç»´åº¦ï¼šå»ºæ¨¡ä¸åŒå˜é‡åœ¨åŒä¸€æ—¶é—´ç‚¹çš„äº¤äº’æ•ˆåº”
    """
    
    def __init__(self, num_variables: int,
                 feature_dim: int,
                 num_heads: int = 4,
                 dropout: float = 0.1,
                 stochastic_dropout: float = 0.1):
        super(SpatioTemporalAttention, self).__init__()
        
        total_dim = num_variables * feature_dim
        
        # æ—¶é—´ç»´åº¦æ³¨æ„åŠ›
        self.temporal_attention = TemporalAttention(
            hidden_dim=total_dim,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # å˜é‡ç»´åº¦æ³¨æ„åŠ›
        self.variable_attention = VariableAttention(
            num_variables=num_variables,
            feature_dim=feature_dim,
            num_heads=num_heads,
            dropout=dropout,
            stochastic_dropout=stochastic_dropout
        )
        
        # èåˆé—¨æ§
        self.gate = nn.Sequential(
            nn.Linear(total_dim * 2, total_dim),
            nn.Sigmoid()
        )
        
    def forward(self, x, return_attention=False):
        """
        Args:
            x: è¾“å…¥å¼ é‡, å½¢çŠ¶ (batch, time_steps, num_variables * feature_dim)
            
        Returns:
            output: æ—¶ç©ºæ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾
            attention_dict: (å¯é€‰) åŒ…å«æ—¶é—´å’Œå˜é‡æ³¨æ„åŠ›æƒé‡çš„å­—å…¸
        """
        # æ—¶é—´æ³¨æ„åŠ›
        temporal_out, temporal_attn = self.temporal_attention(
            x, return_attention=True
        )
        
        # å˜é‡æ³¨æ„åŠ›
        variable_out, variable_attn = self.variable_attention(
            x, return_attention=True
        )
        
        # é—¨æ§èåˆ
        combined = torch.cat([temporal_out, variable_out], dim=-1)
        gate = self.gate(combined)
        output = gate * temporal_out + (1 - gate) * variable_out
        
        if return_attention:
            return output, {
                'temporal': temporal_attn,
                'variable': variable_attn,
                'gate': gate
            }
        return output
```

---

## ğŸ¨ æ³¨æ„åŠ›å¯è§†åŒ–å·¥å…·

```python
# æ–‡ä»¶: src/utils/attention_visualization.py

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

class AttentionVisualizer:
    """æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–å·¥å…·"""
    
    def __init__(self, variable_names: list = None):
        self.variable_names = variable_names
    
    def plot_variable_attention(self, attention_weights, 
                                 title="å˜é‡é—´æ³¨æ„åŠ›æƒé‡",
                                 save_path=None):
        """å¯è§†åŒ–å˜é‡é—´æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
        
        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ (num_vars, num_vars)
            title: å›¾è¡¨æ ‡é¢˜
            save_path: ä¿å­˜è·¯å¾„
        """
        if isinstance(attention_weights, torch.Tensor):
            attention_weights = attention_weights.detach().cpu().numpy()
        
        # å¦‚æœæœ‰å¤šå¤´ï¼Œå–å¹³å‡
        if len(attention_weights.shape) > 2:
            attention_weights = attention_weights.mean(axis=0)
        
        plt.figure(figsize=(10, 8))
        
        labels = self.variable_names if self.variable_names else \
                 [f'Var {i}' for i in range(attention_weights.shape[0])]
        
        sns.heatmap(
            attention_weights,
            xticklabels=labels,
            yticklabels=labels,
            annot=True,
            fmt='.3f',
            cmap='Blues',
            square=True
        )
        
        plt.title(title, fontsize=14)
        plt.xlabel('Key å˜é‡', fontsize=12)
        plt.ylabel('Query å˜é‡', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
    
    def plot_temporal_attention(self, attention_weights,
                                 timestamps=None,
                                 title="æ—¶é—´æ³¨æ„åŠ›æƒé‡",
                                 save_path=None):
        """å¯è§†åŒ–æ—¶é—´æ³¨æ„åŠ›æƒé‡
        
        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ (time, time) æˆ– (heads, time, time)
        """
        if isinstance(attention_weights, torch.Tensor):
            attention_weights = attention_weights.detach().cpu().numpy()
        
        # å–å¹³å‡ï¼ˆå¦‚æœæœ‰å¤šå¤´ï¼‰
        if len(attention_weights.shape) > 2:
            attention_weights = attention_weights.mean(axis=0)
        
        plt.figure(figsize=(12, 10))
        
        sns.heatmap(
            attention_weights,
            cmap='viridis',
            square=True
        )
        
        plt.title(title, fontsize=14)
        plt.xlabel('Key æ—¶é—´æ­¥', fontsize=12)
        plt.ylabel('Query æ—¶é—´æ­¥', fontsize=12)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
    
    def plot_variable_importance_over_time(self, attention_weights_list,
                                            timestamps,
                                            title="å˜é‡é‡è¦æ€§éšæ—¶é—´å˜åŒ–"):
        """ç»˜åˆ¶å„å˜é‡é‡è¦æ€§éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿"""
        # TODO: å®ç°æ—¶åºå˜é‡é‡è¦æ€§å¯è§†åŒ–
        pass
```

---

## ğŸ“ é…ç½®å‚æ•°è¯´æ˜

```yaml
# configs/default_config.yaml
attention:
  type: "spatiotemporal"          # variable, temporal, spatiotemporal
  num_heads: 4                    # æ³¨æ„åŠ›å¤´æ•°
  dropout: 0.1                    # æ ‡å‡†Dropout
  stochastic_dropout: 0.1         # éšæœºæ³¨æ„åŠ›æ­£åˆ™åŒ–ä¸¢å¼ƒç‡
  use_layer_norm: true            # æ˜¯å¦ä½¿ç”¨LayerNorm
  use_residual: true              # æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å› æœæ€§**: æ—¶é—´æ³¨æ„åŠ›å¿…é¡»ä½¿ç”¨å› æœæ©ç ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²
2. **æ­£åˆ™åŒ–**: éšæœºæ³¨æ„åŠ›ä¸¢å¼ƒä»…åœ¨è®­ç»ƒæ—¶å¯ç”¨
3. **å¯è§£é‡Šæ€§**: ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºåç»­åˆ†æ
4. **æ•°å€¼ç¨³å®š**: Softmaxå‰é™¤ä»¥âˆšd_kï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
5. **å¤´æ•°é€‰æ‹©**: å¤´æ•°éœ€æ•´é™¤ç‰¹å¾ç»´åº¦
